{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OzGrav_VItamin_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hagabbar/OzGrav_demo/blob/master/OzGrav_VItamin_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzz4HG9RKqfV",
        "colab_type": "code",
        "outputId": "4e27cc6a-d3fb-4e16-e889-716c7bda868f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!git pull "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)   \rUnpacking objects:  66% (2/3)   \rUnpacking objects: 100% (3/3)   \rUnpacking objects: 100% (3/3), done.\n",
            "From https://github.com/hagabbar/OzGrav_demo\n",
            "   a82ae1b..c323a78  master     -> origin/master\n",
            "Updating a82ae1b..c323a78\n",
            "Fast-forward\n",
            " VICI_code_usage_example.py | 12 \u001b[32m+++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 11 insertions(+), 1 deletion(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdoMTkVPVL6E",
        "colab_type": "text"
      },
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHVnnfXANukP",
        "colab_type": "code",
        "outputId": "076ec457-639c-4fe0-f632-cf5860a94aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!git clone https://github.com/hagabbar/OzGrav_demo.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OzGrav_demo'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 125 (delta 25), reused 22 (delta 10), pack-reused 82\u001b[K\n",
            "Receiving objects: 100% (125/125), 113.68 MiB | 28.82 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tA7BWiUN1NK",
        "colab_type": "code",
        "outputId": "02e42e22-0c17-4f15-fa3a-ec6b62c3a541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "%cd OzGrav_demo/\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/OzGrav_demo\n",
            "bilby_pe.py         OzGrav_VItamin_demo.ipynb  \u001b[0m\u001b[01;34mtest_sets\u001b[0m/\n",
            "LICENSE             plotsky.py                 \u001b[01;34mtraining_sets_3det_9par_256Hz\u001b[0m/\n",
            "make_test_plots.py  plots.py                   VICI_code_usage_example.py\n",
            "\u001b[01;34mModels\u001b[0m/             README.md\n",
            "\u001b[01;34mNeural_Networks\u001b[0m/    requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI04ILUsRgRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93ded0fb-579b-4891-d312-8ce22eaa2694"
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.9.0)\n",
            "Collecting asn1crypto==0.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.8.1)\n",
            "Collecting astroplan==0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/77/f0de165899dd08fdfb89114ab8254a24a185f413e28bfab9ce7a1a54b1a4/astroplan-0.5.tar.gz (266kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 10.7MB/s \n",
            "\u001b[?25hCollecting astropy==3.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/10/6e79f7b5ee8eb0a9210e1b3b950257ead3bdfbb1c8c29208bbead4fbecef/astropy-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 8.9MB/s \n",
            "\u001b[?25hCollecting astropy-healpix==0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/1c/0fb551d7fca83a1dfc337c49421f60ccdf9a55aa7e9d636ca57160353eb5/astropy_healpix-0.4-cp36-cp36m-manylinux1_x86_64.whl (202kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 27.8MB/s \n",
            "\u001b[?25hCollecting astroquery==0.3.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/30/4e93561fa1426a0e1e93dcb7ed05405968fc2792c49d43da023620c9bbc4/astroquery-0.3.10.tar.gz (4.4MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4MB 42.4MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.4MB/s \n",
            "\u001b[?25hCollecting bilby==0.5.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/c3/c9fe5be9ce7ed099fd7d8ea6e0a9e606ae1d8637b541b74c62251ec40c3f/bilby-0.5.5.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 38.7MB/s \n",
            "\u001b[?25hCollecting cachetools==4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
            "Collecting certifi==2019.9.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 41.0MB/s \n",
            "\u001b[?25hCollecting cffi==1.12.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/bf/6aa1925384c23ffeb579e97a5569eb9abce41b6310b329352b8252cee1c3/cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl (430kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (3.0.4)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (7.1.2)\n",
            "Collecting cloudpickle==1.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Collecting corner==2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/65/af/a7ba022f2d5787f51db91b5550cbe8e8c40a6eebd8f15119e743a09a9c19/corner-2.0.1.tar.gz\n",
            "Collecting cpnest==0.9.9\n",
            "  Downloading https://files.pythonhosted.org/packages/03/5c/3df50eb4e7a88aeee45fb4bd34825fbb1273006b88f04ee548659c6504f2/cpnest-0.9.9.tar.gz\n",
            "Collecting cryptography==2.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (0.10.0)\n",
            "Collecting Cython==0.29.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/54/9d66ee2180776dfe33c2b8cc2f2b67c343fd9d80de91ac0edc5bc346fb06/Cython-0.29.15-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 40.4MB/s \n",
            "\u001b[?25hCollecting decorator==4.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/88/0075e461560a1e750a0dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl\n",
            "Collecting deepdish==0.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/39/2a47c852651982bc5eb39212ac110284dd20126bdc7b49bde401a0139f5d/deepdish-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dill==0.3.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (0.3.1.1)\n",
            "Collecting dqsegdb2==1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/97/355617233a3b4a25966b35a536379d23345e9783db009248a199a48e60dd/dqsegdb2-1.0.1-py2.py3-none-any.whl\n",
            "Collecting dynesty==0.9.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/0b/78555fafdbfe9f13771fbedfd0aacf574c4eaf4325ba932c93fa883daa1b/dynesty-0.9.7-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.3MB/s \n",
            "\u001b[?25hCollecting emcee==2.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/d3/7635106605dedccd08705beac53be4c43a8da1caad6be667adbf93ed0965/emcee-2.2.1.tar.gz\n",
            "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (0.3)\n",
            "Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 28)) (1.1.2)\n",
            "Collecting future==0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 37.4MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting google-auth==1.11.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 32)) (0.4.1)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 33)) (0.2.0)\n",
            "Collecting grpcio==1.27.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/df/1f8a284a5e5819ae07d50bd76996d6f7208afef7533e4896fa1c6445574f/grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 39.1MB/s \n",
            "\u001b[?25hCollecting gwdatafind==1.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/9e/616124723b7a8f2a5399f9288b056bb31f37222b0b7cae46f6bafaa42154/gwdatafind-1.0.4-py2.py3-none-any.whl\n",
            "Collecting gwosc==0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/ff/67426ce11f9f3432e020f5d5ef796e1d8e1c2a76b555c8705177f4347f99/gwosc-0.4.3-py2.py3-none-any.whl\n",
            "Collecting gwpy==0.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/e6/14f8cefc3ebd73e062323ed94ecc2ea943b3ab69137b2b9fc9df4087a9bc/gwpy-0.15.0-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 37.8MB/s \n",
            "\u001b[?25hCollecting h5py==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 38.0MB/s \n",
            "\u001b[?25hCollecting healpy==1.12.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/49/573b66cd7e8b4aedb987c0635a61e2af88f8a2963db5202bb2c3236ae80e/healpy-1.12.10-cp36-cp36m-manylinux1_x86_64.whl (13.5MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5MB 150kB/s \n",
            "\u001b[?25hRequirement already satisfied: html5lib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 40)) (1.0.1)\n",
            "Collecting idna==2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata==1.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 42)) (1.6.0)\n",
            "Collecting importlib-resources==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/2d/88f166bcaadc09d9fdbf1c336ad118e01b7fe1155e15675e125be2ff1899/importlib_resources-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 44)) (1.1.0)\n",
            "Collecting jeepney==0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/4c/ef880713a6c6d628869596703167eab2edf8e0ec2d870d1089dcb0901b81/jeepney-0.4.1-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2==2.11.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 46)) (2.11.2)\n",
            "Collecting joblib==0.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 48)) (2.3.1)\n",
            "Requirement already satisfied: Keras-Applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 49)) (1.0.8)\n",
            "Collecting Keras-Preprocessing==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hCollecting keyring==19.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/08/ad1ae7262c8146bee3be360cc766d0261037a90b44872b080a53aaed4e84/keyring-19.2.0-py2.py3-none-any.whl\n",
            "Collecting kiwisolver==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[?25hCollecting lalsuite==6.62\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/af/cc935da87397aab1229c2f84868e9981ec854f1569ef552b4330d1363479/lalsuite-6.62-cp36-cp36m-manylinux1_x86_64.whl (30.3MB)\n",
            "\u001b[K     |████████████████████████████████| 30.3MB 87kB/s \n",
            "\u001b[?25hCollecting ligo-gracedb==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/db73aeabaf2f5494b7c567d021ff932e33771c3673442c9400fe1f32c735/ligo_gracedb-2.4.0-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 38.7MB/s \n",
            "\u001b[?25hCollecting ligo-segments==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/cd/225e331e95cf6aff8ba13bf9a8053b29248a5e71f7fa9bbb1f0db1eaadff/ligo-segments-1.2.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hCollecting ligo.skymap==0.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/87/bb184bf7b8fa51a478de8b16f3a6c7eb4236b28aa8a725acc4dff2d95d51/ligo.skymap-0.3.1-cp36-cp36m-manylinux2014_x86_64.whl (22.2MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2MB 46.4MB/s \n",
            "\u001b[?25hCollecting ligotimegps==2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/69/b6/6d6d0585fa2ae936a9f5d411b1f0fbe9fcb0aca0c51a775aa4f8f95fdf5e/ligotimegps-2.0.1-py2.py3-none-any.whl\n",
            "Collecting lscsoft-glue==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/65/e93853bc1876516db8d58f4590dba1d6b85eaf9d1bd375926ac7897e525a/lscsoft-glue-2.0.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 37.0MB/s \n",
            "\u001b[?25hCollecting lxml==4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/6f/c87dffdd88a54dd26a3a9fef1d14b6384a9933c455c54ce3ca7d64a84c88/lxml-4.5.1-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 32.3MB/s \n",
            "\u001b[?25hCollecting Markdown==3.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 61)) (1.1.1)\n",
            "Requirement already satisfied: matplotlib==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 62)) (3.2.1)\n",
            "Collecting mock==3.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Collecting networkx==2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 30.9MB/s \n",
            "\u001b[?25hCollecting numexpr==2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/5e/ee657b36ce1b6baabaafe485e97a31e7200f918c4b8643ebc4fd4fd07ada/numexpr-2.7.0-cp36-cp36m-manylinux1_x86_64.whl (162kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 42.6MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 91kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 67)) (3.1.0)\n",
            "Collecting opt-einsum==3.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/49/2233e63052d5686c72131b579837ddfb98ba9dd0b92bb91efcb441ada8ce/opt_einsum-3.2.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[?25hCollecting pandas==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ec/b5dd8cfb078380fb5ae9325771146bccd4e8cad2d3e4c72c7433010684eb/pandas-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 24.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 70)) (0.5.1)\n",
            "Collecting Pillow==6.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/41/db6dec65ddbc176a59b89485e8cc136a433ed9c6397b6bfe2cd38412051e/Pillow-6.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 25.3MB/s \n",
            "\u001b[?25hCollecting protobuf==3.11.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 35.8MB/s \n",
            "\u001b[?25hCollecting ptemcee==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/81/1d/2b3f09f0eb3ce8530e59b03e1aab4bc260cc14ddbc7dad761f9107d944e6/ptemcee-1.0.0.tar.gz\n",
            "Collecting pyaml==20.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 75)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 76)) (0.2.8)\n",
            "Collecting pycparser==2.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymc3==3.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 78)) (3.7)\n",
            "Collecting pyOpenSSL==19.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/bc/1e58593167fade7b544bfe9502a26dc860940a79ab306e651e7f13be68c2/pyparsing-2.4.6-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hCollecting pyproj==2.6.1.post1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c3/071e080230ac4b6c64f1a2e2f9161c9737a2bc7b683d2c90b024825000c0/pyproj-2.6.1.post1-cp36-cp36m-manylinux2010_x86_64.whl (10.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9MB 32.0MB/s \n",
            "\u001b[?25hCollecting pyshp==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/16/3bf15aa864fb77845fab8007eda22c2bd67bd6c1fd13496df452c8c43621/pyshp-2.1.0.tar.gz (215kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 83)) (2.8.1)\n",
            "Collecting python-ligo-lw==1.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/e1/bfde0061183bec90ec77ff152959d285e784df28dcccfbb3f12659509d71/python-ligo-lw-1.5.3.tar.gz (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 36.9MB/s \n",
            "\u001b[?25hCollecting pytz==2019.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 32.1MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 45.4MB/s \n",
            "\u001b[?25hCollecting reproject==0.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/27/ff9326889df12fc93b336c2e4c4f5491f137358465645ff51d8c594a2924/reproject-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (392kB)\n",
            "\u001b[K     |████████████████████████████████| 399kB 38.7MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 89)) (1.3.0)\n",
            "Requirement already satisfied: rsa==4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 90)) (4.0)\n",
            "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 91)) (0.22.2.post1)\n",
            "Collecting scikit-optimize==0.7.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/87/310b52debfbc0cb79764e5770fa3f5c18f6f0754809ea9e2fc185e1b67d3/scikit_optimize-0.7.4-py2.py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 93)) (1.4.1)\n",
            "Collecting seaborn==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 43.3MB/s \n",
            "\u001b[?25hCollecting SecretStorage==3.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/59/cb226752e20d83598d7fdcabd7819570b0329a61db07cfbdd21b2ef546e3/SecretStorage-3.1.1-py3-none-any.whl\n",
            "Collecting six==1.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
            "Collecting soupsieve==1.9.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/44/0474f2207fdd601bb25787671c81076333d2c80e6f97e92790f8887cf682/soupsieve-1.9.3-py2.py3-none-any.whl\n",
            "Collecting tables==3.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/f7/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 30.1MB/s \n",
            "\u001b[?25hCollecting tensorboard==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 31.3MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 24kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 73kB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/ed/f587d64127bbb85e539f06a2aace1240b7b5c6b4267bea94f232230551a5/tensorflow_probability-0.9.0-py2.py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 103)) (1.1.0)\n",
            "Requirement already satisfied: Theano==1.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 104)) (1.0.4)\n",
            "Collecting tqdm==4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/80/5bb262050dd2f30f8819626b7c92339708fe2ed7bd5554c8193b4487b367/tqdm-4.42.1-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hCollecting universal-divergence==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/58/57/5f5e30e6ff1c6d63fb925517fcc467325bfa82f9a2e81a39bc69197e76d1/universal-divergence-0.2.0.tar.gz\n",
            "Collecting urllib3==1.25.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/da/55f51ea951e1b7c63a579c09dd7db825bb730ec1fe9c0180fc77bfb31448/urllib3-1.25.6-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 108)) (0.5.1)\n",
            "Collecting Werkzeug==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/a5/d6f8a6e71f15364d35678a4ec8a0186f980b3bd2545f40ad51dd26a87fb1/Werkzeug-1.0.0-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 110)) (1.12.1)\n",
            "Requirement already satisfied: zipp==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 111)) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.11.3->-r requirements.txt (line 31)) (47.1.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.1->-r requirements.txt (line 99)) (0.34.2)\n",
            "Building wheels for collected packages: astroplan, astroquery, bilby, corner, cpnest, emcee, future, gast, ligo-segments, lscsoft-glue, networkx, ptemcee, pycparser, pyshp, python-ligo-lw, PyYAML, universal-divergence\n",
            "  Building wheel for astroplan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astroplan: filename=astroplan-0.5-cp36-none-any.whl size=99603 sha256=da5a62825a7a61c3b4ca534edc567e88ff41f45d55a52fb2a21edb7ea8ae5683\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/95/09/9ee021c34394bdfa2e52f09019ed8e3f9ddf56178016d97d6c\n",
            "  Building wheel for astroquery (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astroquery: filename=astroquery-0.3.10-cp36-none-any.whl size=3508442 sha256=475c77c3c46aba83966086034e2832ca1b9b4359bd41bbdfc89b929bf4063361\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/2f/df/06562d703b7f0577221c87174ae12bf4dcab0d40e8a02a8988\n",
            "  Building wheel for bilby (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bilby: filename=bilby-0.5.5-cp36-none-any.whl size=1545905 sha256=e0ee832c1591742a9b2f349859a7bc8f8b9aa2c8d6540e95bb0c4ffada6cf839\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/ac/45/1658132d8a778e2486b33a5051d1f9f3b5518e0359ce5874b2\n",
            "  Building wheel for corner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for corner: filename=corner-2.0.1-cp36-none-any.whl size=11642 sha256=f64da5324f470991551e24a659dafcf998133e1d87f610e888ce98d3ed4e4bd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/d8/e5/e0e7974a2a5757483ea5a180c937041cf6872dc9993d78234a\n",
            "  Building wheel for cpnest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cpnest: filename=cpnest-0.9.9-cp36-cp36m-linux_x86_64.whl size=154761 sha256=d889bb2fb8880e810ad25b29f1c0c26af4c820f7a580ac238dcd6147559a77fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/86/37/685ea633db7771e3d20766b80132b2dded7e5104c82aba3161\n",
            "  Building wheel for emcee (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emcee: filename=emcee-2.2.1-cp36-none-any.whl size=29597 sha256=eeff23572ff3b32ddf3111d71dad93df3579a93caccfda043165e143fba49a1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/5d/a5/78f84e23329ad7d9b1787c9d24371100cae74cdefe25eba50d\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=3d5ee2b7b5c70493017152351b246b1d13e6639e814f5706e3a29cf21929dd77\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=94498c8ccc3a4e1a62ffc8f6c57da1d8da2a1c5df1a23a473652a6809db051b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for ligo-segments (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ligo-segments: filename=ligo_segments-1.2.0-cp36-cp36m-linux_x86_64.whl size=83606 sha256=d63069e71d881e375bdb95bfa4b3d0208366f43dbd8f7c0231f6dd6aef734860\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/1e/4a/ab4122baed7d67f6abce65b2b12049d3bc7fe5dad24edf89df\n",
            "  Building wheel for lscsoft-glue (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lscsoft-glue: filename=lscsoft_glue-2.0.0-cp36-cp36m-linux_x86_64.whl size=414157 sha256=c1b9f6e2c52af1ac406308467b0bca14d3940e26a0f91017621cf34e41a4301d\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/fa/38/d61b002c627ca54f03755b9a288f4b1fa83291608a4bc47b7b\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556408 sha256=5f8fffb17a62eac1b84b7ea966c25a55313eefccc4a85d8a8d8974087d72eff5\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n",
            "  Building wheel for ptemcee (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptemcee: filename=ptemcee-1.0.0-cp36-none-any.whl size=18551 sha256=e9b9149dbc9eb844f718f5d458856bf063efde36f49a1dbfe26d459b0bf7787d\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/89/43/424c452a5548876bab54b4c57532a19e6557b48f9651e8c79c\n",
            "  Building wheel for pycparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111031 sha256=2d078c4bdd682548d68daf428b37853f1c7191224129cb4d9f5758c9e0e19098\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
            "  Building wheel for pyshp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyshp: filename=pyshp-2.1.0-cp36-none-any.whl size=32609 sha256=ee14f0556c58ac3ea2a19ffac0bbb8f7dceea0ab79919f878a8f3c4c6da76a6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/0c/de/321b5192ad416b328975a2f0385f72c64db4656501eba7cc1a\n",
            "  Building wheel for python-ligo-lw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-ligo-lw: filename=python_ligo_lw-1.5.3-cp36-cp36m-linux_x86_64.whl size=178178 sha256=53d78b5172ae0fbccfea638286e6be4edfeda031096b69606ac585dbd1a96a5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/7d/0f/9852ed7aeb135467f9aa828b57b3118f02e3ece9ae10b3b29e\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=92b217d269a69c210d40f33aa570feda4e6b70fa53bf73fc7d858ac4c309d7e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "  Building wheel for universal-divergence (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for universal-divergence: filename=universal_divergence-0.2.0-cp36-none-any.whl size=2831 sha256=e01351a370aff81a9a97c5914c255b389675dbabef1c6926e000ce8335edc0a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/a0/ab/1aae4a117337edafc4ad5d24340cb381487d23b35dc1ed75f3\n",
            "Successfully built astroplan astroquery bilby corner cpnest emcee future gast ligo-segments lscsoft-glue networkx ptemcee pycparser pyshp python-ligo-lw PyYAML universal-divergence\n",
            "\u001b[31mERROR: rpy2 3.2.7 has requirement cffi>=1.13.1, but you'll have cffi 1.12.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.7.2, but you'll have google-auth 1.11.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: asn1crypto, numpy, astropy, pytz, astroplan, astropy-healpix, certifi, idna, urllib3, requests, jeepney, pycparser, cffi, six, cryptography, SecretStorage, keyring, soupsieve, beautifulsoup4, astroquery, future, dynesty, corner, pandas, bilby, cachetools, cloudpickle, tqdm, Cython, cpnest, decorator, numexpr, mock, tables, deepdish, ligo-segments, pyOpenSSL, gwdatafind, dqsegdb2, emcee, gast, google-auth, grpcio, gwosc, h5py, ligotimegps, gwpy, healpy, importlib-resources, joblib, Keras-Preprocessing, kiwisolver, lscsoft-glue, lalsuite, ligo-gracedb, Pillow, ptemcee, networkx, PyYAML, python-ligo-lw, reproject, ligo.skymap, lxml, Markdown, opt-einsum, protobuf, pyaml, pyparsing, pyproj, pyshp, scikit-optimize, seaborn, Werkzeug, tensorboard, tensorflow-estimator, tensorflow, tensorflow-probability, universal-divergence\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: astropy 4.0.1.post1\n",
            "    Uninstalling astropy-4.0.1.post1:\n",
            "      Successfully uninstalled astropy-4.0.1.post1\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: certifi 2020.4.5.1\n",
            "    Uninstalling certifi-2020.4.5.1:\n",
            "      Successfully uninstalled certifi-2020.4.5.1\n",
            "  Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: pycparser 2.20\n",
            "    Uninstalling pycparser-2.20:\n",
            "      Successfully uninstalled pycparser-2.20\n",
            "  Found existing installation: cffi 1.14.0\n",
            "    Uninstalling cffi-1.14.0:\n",
            "      Successfully uninstalled cffi-1.14.0\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: pandas 1.0.4\n",
            "    Uninstalling pandas-1.0.4:\n",
            "      Successfully uninstalled pandas-1.0.4\n",
            "  Found existing installation: cachetools 3.1.1\n",
            "    Uninstalling cachetools-3.1.1:\n",
            "      Successfully uninstalled cachetools-3.1.1\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: Cython 0.29.19\n",
            "    Uninstalling Cython-0.29.19:\n",
            "      Successfully uninstalled Cython-0.29.19\n",
            "  Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Found existing installation: numexpr 2.7.1\n",
            "    Uninstalling numexpr-2.7.1:\n",
            "      Successfully uninstalled numexpr-2.7.1\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: google-auth 1.7.2\n",
            "    Uninstalling google-auth-1.7.2:\n",
            "      Successfully uninstalled google-auth-1.7.2\n",
            "  Found existing installation: grpcio 1.29.0\n",
            "    Uninstalling grpcio-1.29.0:\n",
            "      Successfully uninstalled grpcio-1.29.0\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: joblib 0.15.1\n",
            "    Uninstalling joblib-0.15.1:\n",
            "      Successfully uninstalled joblib-0.15.1\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: kiwisolver 1.2.0\n",
            "    Uninstalling kiwisolver-1.2.0:\n",
            "      Successfully uninstalled kiwisolver-1.2.0\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: networkx 2.4\n",
            "    Uninstalling networkx-2.4:\n",
            "      Successfully uninstalled networkx-2.4\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "  Found existing installation: Markdown 3.2.2\n",
            "    Uninstalling Markdown-3.2.2:\n",
            "      Successfully uninstalled Markdown-3.2.2\n",
            "  Found existing installation: opt-einsum 3.2.1\n",
            "    Uninstalling opt-einsum-3.2.1:\n",
            "      Successfully uninstalled opt-einsum-3.2.1\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Found existing installation: seaborn 0.10.1\n",
            "    Uninstalling seaborn-0.10.1:\n",
            "      Successfully uninstalled seaborn-0.10.1\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Found existing installation: tensorflow-probability 0.10.0\n",
            "    Uninstalling tensorflow-probability-0.10.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.10.0\n",
            "Successfully installed Cython-0.29.15 Keras-Preprocessing-1.1.0 Markdown-3.2.1 Pillow-6.1.0 PyYAML-5.1.2 SecretStorage-3.1.1 Werkzeug-1.0.0 asn1crypto-0.24.0 astroplan-0.5 astropy-3.2.1 astropy-healpix-0.4 astroquery-0.3.10 beautifulsoup4-4.8.0 bilby-0.5.5 cachetools-4.0.0 certifi-2019.9.11 cffi-1.12.3 cloudpickle-1.2.2 corner-2.0.1 cpnest-0.9.9 cryptography-2.7 decorator-4.4.0 deepdish-0.3.6 dqsegdb2-1.0.1 dynesty-0.9.7 emcee-2.2.1 future-0.18.2 gast-0.2.2 google-auth-1.11.3 grpcio-1.27.2 gwdatafind-1.0.4 gwosc-0.4.3 gwpy-0.15.0 h5py-2.9.0 healpy-1.12.10 idna-2.8 importlib-resources-1.5.0 jeepney-0.4.1 joblib-0.14.1 keyring-19.2.0 kiwisolver-1.1.0 lalsuite-6.62 ligo-gracedb-2.4.0 ligo-segments-1.2.0 ligo.skymap-0.3.1 ligotimegps-2.0.1 lscsoft-glue-2.0.0 lxml-4.5.1 mock-3.0.5 networkx-2.3 numexpr-2.7.0 numpy-1.18.4 opt-einsum-3.2.0 pandas-1.0.1 protobuf-3.11.3 ptemcee-1.0.0 pyOpenSSL-19.0.0 pyaml-20.4.0 pycparser-2.19 pyparsing-2.4.6 pyproj-2.6.1.post1 pyshp-2.1.0 python-ligo-lw-1.5.3 pytz-2019.3 reproject-0.5.1 requests-2.22.0 scikit-optimize-0.7.4 seaborn-0.9.0 six-1.14.0 soupsieve-1.9.3 tables-3.5.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 tensorflow-probability-0.9.0 tqdm-4.42.1 universal-divergence-0.2.0 urllib3-1.25.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cachetools",
                  "certifi",
                  "cffi",
                  "cloudpickle",
                  "decorator",
                  "google",
                  "grpc",
                  "idna",
                  "kiwisolver",
                  "numpy",
                  "pandas",
                  "pyparsing",
                  "pytz",
                  "requests",
                  "six",
                  "tqdm",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naVHRD3ugIol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtFV8KXBpnI1",
        "colab_type": "text"
      },
      "source": [
        "# Running VItamin on your own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXMU1DTWoep",
        "colab_type": "text"
      },
      "source": [
        "## Generate Bilby Posterior Test Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhhXcM5NW92Q",
        "colab_type": "text"
      },
      "source": [
        "## Generate Traning Samples\n",
        "\n",
        "Demo default parameters are set to generate ~10,000 training samples, though for best results greater than 1 million samples is recommened (ideally 10 million)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3euU5D6zXCGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --gen_train True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XmOPmHJ3PT1u"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "Training utilizes the K80 GPU available to use on the google colab tutorial for free. In the paper we use the NVIDIA DGX1 machine GPUs (just one) at the detector sites. Large convolutional models can take up to 8 - 16 Gb of GPU memory, so it is recommended to use as high end of a GPU as is available to you.\n",
        "\n",
        "Corner plots are generated every 50,000 iterations in the results directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TN5F_GVfPkWQ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgMNjoMhPrHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --train True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeE98hRnXadJ",
        "colab_type": "text"
      },
      "source": [
        "## Compare Bilby vs. VItamin\n",
        "\n",
        "This will produce three sets of plots. Corner plots comparing VItamin samples with all other requested Bayesian samples. KL divergence plots showing the KL divergence between VItamin and Bayesian samplers (ideally we should see that all distributions are similar). P-P plots which show how generally consistent with the truth each sampler is (ideally all samplers line up along the diagonal). Plots are stored in the results folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sGBzCe9AyB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --test True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZdzHYaSVPvb",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "This tutorial is based on the paper Bayesian Parameter Estimation using Conditional Variational Autoencoders for Gravitational-wave Astronomy (https://arxiv.org/abs/1909.06296). In the Background section, I will provide some description on the inner workings of the VItamin code. In the Running VItamin section, you will be able to generate your own data and run your own neural network to produce GW posteriors. \n",
        "\n",
        "Note: Code in Background section is NOT executable. Running VItamin section IS executable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvTJ5Js5ouz0",
        "colab_type": "text"
      },
      "source": [
        "## Network Training Portion of VItamin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0mWXk0Lzduz",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow 1.0 requires that we define a TensorFlow session where the computational graph will execute. This is not necessary to do in TensorFlow 2.0. We also need to define place holders for future inputs to be used when we eventually go to train the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMOPeFqezZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "session = tf.Session(graph=graph)\n",
        "with graph.as_default():\n",
        "\n",
        "    # PLACE HOLDERS\n",
        "    bs_ph = tf.placeholder(dtype=tf.int64, name=\"bs_ph\")                        # batch size placeholder\n",
        "    x_ph = tf.placeholder(dtype=tf.float32, shape=[None, xsh[1]], name=\"x_ph\")  # params placeholder\n",
        "    if n_conv_r1 != None:\n",
        "        if params['by_channel'] == True:\n",
        "            y_ph = tf.placeholder(dtype=tf.float32, shape=[None,ysh,len(fixed_vals['det'])], name=\"y_ph\") # data placeholder\n",
        "        else:\n",
        "            y_ph = tf.placeholder(dtype=tf.float32, shape=[None,len(fixed_vals['det']),ysh], name=\"y_ph\") # data placeholder\n",
        "    else:\n",
        "        y_ph = tf.placeholder(dtype=tf.float32, shape=[None,ysh], name=\"y_ph\")  # data placeholder\n",
        "    idx = tf.placeholder(tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsbJ-xY3z4G0",
        "colab_type": "text"
      },
      "source": [
        "Define and import from seperate scripts the three networks we will be training. (i.e. r1,r2 and q)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMSaz_kw0Fqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # LOAD VICI NEURAL NETWORKS\n",
        "    r2_xzy = VICI_decoder.VariationalAutoencoder('VICI_decoder', wrap_mask, nowrap_mask, \n",
        "                                                  n_input1=z_dimension, n_input2=ysh_conv_r2, n_output=xsh[1], \n",
        "                                                  n_weights=n_weights_r2, n_hlayers=n_hlayers_r2, \n",
        "                                                  drate=drate, n_filters=n_filters_r2, filter_size=filter_size_r2,\n",
        "                                                  maxpool=maxpool_r2, n_conv=n_conv_r2, conv_strides=conv_strides_r2, pool_strides=pool_strides_r2, num_det=num_det, batch_norm=batch_norm, by_channel=params['by_channel'], weight_init=params['weight_init'])\n",
        "    r1_zy = VICI_encoder.VariationalAutoencoder('VICI_encoder', n_input=ysh_conv_r1, n_output=z_dimension, \n",
        "                                                  n_weights=n_weights_r1, n_modes=n_modes, \n",
        "                                                  n_hlayers=n_hlayers_r1, drate=drate, n_filters=n_filters_r1, \n",
        "                                                  filter_size=filter_size_r1,maxpool=maxpool_r1, n_conv=n_conv_r1, conv_strides=conv_strides_r1, pool_strides=pool_strides_r1, num_det=num_det, batch_norm=batch_norm, by_channel=params['by_channel'], weight_init=params['weight_init'])\n",
        "    q_zxy = VICI_VAE_encoder.VariationalAutoencoder('VICI_VAE_encoder', n_input1=xsh[1], n_input2=ysh_conv_q, \n",
        "                                                    n_output=z_dimension, n_weights=n_weights_q, \n",
        "                                                    n_hlayers=n_hlayers_q, drate=drate, n_filters=n_filters_q, \n",
        "                                                    filter_size=filter_size_q,maxpool=maxpool_q, n_conv=n_conv_q, conv_strides=conv_strides_q, pool_strides=pool_strides_q, num_det=num_det, batch_norm=batch_norm, by_channel=params['by_channel'], weight_init=params['weight_init']) # used to sample from q(z|x,y)?\n",
        "    tf.set_random_seed(np.random.randint(0,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hXt7oht8foX",
        "colab_type": "text"
      },
      "source": [
        "Apply a ramp which is linear in log space on the Kubler-Leibeck divergence term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43mki4md854f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    ramp = (tf.log(tf.dtypes.cast(idx,dtype=tf.float32)) - tf.log(ramp_start))/(tf.log(ramp_end)-tf.log(ramp_start))\n",
        "    ramp = tf.minimum(tf.math.maximum(0.0,ramp),1.0)\n",
        "        \n",
        "    if params['ramp'] == False:\n",
        "        ramp = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21nGKlh688GO",
        "colab_type": "text"
      },
      "source": [
        "Get the encoder network r1(z|y) latent predicted mean, log variances and mode weights of latent space Gaussian distributions. (i.e. r1(z|y))\n",
        "\n",
        "*   `r1_loc.shape` = `(batch_size, modes, latent_space_dimensions)`\n",
        "*   `r1_scale.shape` = `(batch_size, modes, latent_space_dimensions)`\n",
        "*   `r1_weight.shape` = `(batch_size, modes)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVMOs-Dt-oVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # reduce the y data size\n",
        "    y_conv = y_ph\n",
        "\n",
        "    # GET r1(z|y)\n",
        "    # run inverse autoencoder to generate mean and logvar of z given y data - these are the parameters for r1(z|y)\n",
        "    r1_loc, r1_scale, r1_weight = r1_zy._calc_z_mean_and_sigma(y_conv)\n",
        "    r1_scale = tf.sqrt(SMALL_CONSTANT + tf.exp(r1_scale))\n",
        "\n",
        "    # apply KL ramp to r1_weight\n",
        "    r1_weight = ramp*tf.squeeze(r1_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNnEI-ZkAwiA",
        "colab_type": "text"
      },
      "source": [
        "Key aspect of code. We define a Gaussian mixture model, given predicted means (`r1_loc`), log variances (`r1_scale`) and mode weights (`r1_weight`) from the r1 network. This alows us to generate multi-modal posteriors.\n",
        "\n",
        "`mixture_distribution` argument tell us the probability that we will draw from a particular mode. `components_distribution` are the components from which we will draw samples.\n",
        "\n",
        "We can then sample from this distribution in order to get out a single sample from each of the r1 latent space dimensions for every training case in batch (r1_zy_samp).\n",
        "\n",
        "\n",
        "\n",
        "*   `r1_zy_samp.shape` = `(batch_size, latent_space_dimensions)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oKmSdWrAwMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # define the r1(z|y) mixture model\n",
        "    bimix_gauss = tfd.MixtureSameFamily(\n",
        "                      mixture_distribution=tfd.Categorical(logits=ramp*r1_weight),\n",
        "                      components_distribution=tfd.MultivariateNormalDiag(\n",
        "                      loc=r1_loc,\n",
        "                      scale_diag=r1_scale))\n",
        "    # DRAW FROM r1(z|y) - given the Gaussian parameters generate z samples\n",
        "    r1_zy_samp = bimix_gauss.sample()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfoUKrGgE7ak",
        "colab_type": "text"
      },
      "source": [
        "Calculate the predicted means and log variances of the q network. Given those means and log variances, retrieve a single sample from each latent space dimension in the form of (`q_zxy_samp`).\n",
        "\n",
        "\n",
        "\n",
        "*   `q_zxy_mean.shape` = `(batch_size, latent_space_dimensions)`\n",
        "*   `q_zxy_log_sig_sq` = `(batch_size, latent_space_dimensions)`\n",
        "*   `q_zxy_samp` = `(batch_size, latent_space_dimensions)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO89n21-E6HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # GET q(z|x,y)\n",
        "    q_zxy_mean, q_zxy_log_sig_sq = q_zxy._calc_z_mean_and_sigma(x_ph,y_conv)\n",
        "\n",
        "    # DRAW FROM q(z|x,y)\n",
        "    q_zxy_samp = q_zxy._sample_from_gaussian_dist(bs_ph, z_dimension, q_zxy_mean, tf.log(SMALL_CONSTANT + tf.exp(q_zxy_log_sig_sq)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzU46Sx6GqZh",
        "colab_type": "text"
      },
      "source": [
        "Retrieve predicted means and standard deviations from r2 network. Identify source parameters to apply Von Mises wrapping to.\n",
        "\n",
        "\n",
        "\n",
        "*   `r2_xzy_mean_nowrap.shape` = `(batch_size, number_unwrapped_parameters)`\n",
        "*   `r2_xzy_log_sig_sq_nowrap.shape` = `(batch_size, number_unwrapped_parameters)`\n",
        "*   `r2_xzy_mean_wrap.shape` = `(batch_size, number_wrapped_parameters)`\n",
        "*   `r2_xzy_log_sig_sq_wrap.shape` = `(batch_size, number_wrapped_parameters)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-o8UNY-GqBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # GET r2(x|z,y)\n",
        "    reconstruction_xzy = r2_xzy.calc_reconstruction(q_zxy_samp,y_conv)\n",
        "    r2_xzy_mean_nowrap = reconstruction_xzy[0]\n",
        "    r2_xzy_log_sig_sq_nowrap = reconstruction_xzy[1]\n",
        "    if np.sum(wrap_mask)>0:\n",
        "        r2_xzy_mean_wrap = reconstruction_xzy[2]\n",
        "        r2_xzy_log_sig_sq_wrap = reconstruction_xzy[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23_r0eA0HbCb",
        "colab_type": "text"
      },
      "source": [
        "Apply a small normalization factor to predicted means of r2 source parameter predictions (for algebra reasons(normalising_factor_x)). Compute the mean difference between predicted means from r2 and source parameter truths (essentially a mean squared error(square_diff_between_mu_and_x)). Divide the mean difference by the standard deviation squared (inside_exp_x). Apply pre-computed small normalization factor (normalising_factor_x) to pre-computed mean difference (inside_exp_x). \n",
        "\n",
        "All this returns the reconstruction loss, or in other words ... how far off the mean locations of the network predicted source parameter Gaussians are from the source parameter truths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rlr-wO7HZjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # COST FROM RECONSTRUCTION - Gaussian parts\n",
        "    normalising_factor_x = -0.5*tf.log(SMALL_CONSTANT + tf.exp(r2_xzy_log_sig_sq_nowrap)) - 0.5*np.log(2.0*np.pi)   # -0.5*log(sig^2) - 0.5*log(2*pi)\n",
        "    square_diff_between_mu_and_x = tf.square(r2_xzy_mean_nowrap - tf.boolean_mask(x_ph,nowrap_mask,axis=1))         # (mu - x)^2\n",
        "\n",
        "    inside_exp_x = -0.5 * tf.divide(square_diff_between_mu_and_x,SMALL_CONSTANT + tf.exp(r2_xzy_log_sig_sq_nowrap)) # -0.5*(mu - x)^2 / sig^2\n",
        "    reconstr_loss_x = tf.reduce_sum(normalising_factor_x + inside_exp_x,axis=1,keepdims=True)                       # sum_dim(-0.5*log(sig^2) - 0.5*log(2*pi) - 0.5*(mu - x)^2 / sig^2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeYePUPaK19a",
        "colab_type": "text"
      },
      "source": [
        "We use Von Mises distribution, rather than a Gaussian, on source parameters we know to have periodic-like distributions. `con` is the concentration of the distribtion (similar to standard deviation parameter in Gaussian). `loc` is the circular mean of the distribution (similar the the mean of a Gaussian).\n",
        "\n",
        "We get out a loss for Von Mises parameters by computing the log probability of the distribution evaluated at the source parameter truths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDpz8NpKKxj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # COST FROM RECONSTRUCTION - Von Mises parts\n",
        "    if np.sum(wrap_mask)>0:\n",
        "        con = tf.reshape(tf.math.reciprocal(SMALL_CONSTANT + tf.exp(r2_xzy_log_sig_sq_wrap)),[-1,wrap_len])                                             # modelling wrapped scale output as log variance\n",
        "        von_mises = tfp.distributions.VonMises(loc=2.0*np.pi*(tf.reshape(r2_xzy_mean_wrap,[-1,wrap_len])-0.5), concentration=con)                       # define p_vm(2*pi*mu,con=1/sig^2)\n",
        "        reconstr_loss_vm = tf.reduce_sum(von_mises.log_prob(2.0*np.pi*(tf.reshape(tf.boolean_mask(x_ph,wrap_mask,axis=1),[-1,wrap_len]) - 0.5)),axis=1) # 2pi is the von mises input range\n",
        "        r2_xzy_mean = tf.gather(tf.concat([r2_xzy_mean_nowrap,r2_xzy_mean_wrap],axis=1),tf.constant(idx_mask),axis=1)\n",
        "        r2_xzy_scale = tf.gather(tf.concat([r2_xzy_log_sig_sq_nowrap,r2_xzy_log_sig_sq_wrap],axis=1),tf.constant(idx_mask),axis=1) \n",
        "        cost_R = -1.0*tf.reduce_mean(reconstr_loss_x + reconstr_loss_vm)                                                                                # average over batch\n",
        "    else:\n",
        "        cost_R = -1.0*tf.reduce_mean(reconstr_loss_x)    \n",
        "        r2_xzy_mean = r2_xzy_mean_nowrap\n",
        "        r2_xzy_scale = r2_xzy_log_sig_sq_nowrap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXssQBJTMh24",
        "colab_type": "text"
      },
      "source": [
        "Compute the level of uncertainty in q. Basically do the same set of operations done on r2 earlier. Compute mean squared difference between predicted means from q and samples of the latent space from q and normalize. Essentially, we don't want the locations of the predicted latent space Gaussian to be all over the place. We want them to settle into a local minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rthZxyn6MhrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # compute montecarlo KL - first compute the analytic self entropy of q \n",
        "    normalising_factor_kl = -0.5*tf.log(SMALL_CONSTANT + tf.exp(q_zxy_log_sig_sq)) - 0.5*np.log(2.0*np.pi)   # -0.5*log(sig^2) - 0.5*log(2*pi)\n",
        "    square_diff_between_qz_and_q = tf.square(q_zxy_mean - q_zxy_samp)                                        # (mu - x)^2\n",
        "    inside_exp_q = -0.5 * tf.divide(square_diff_between_qz_and_q,SMALL_CONSTANT + tf.exp(q_zxy_log_sig_sq))  # -0.5*(mu - x)^2 / sig^2\n",
        "    log_q_q = tf.reduce_sum(normalising_factor_kl + inside_exp_q,axis=1,keepdims=True)                       # sum_dim(-0.5*log(sig^2) - 0.5*log(2*pi) - 0.5*(mu - x)^2 / sig^2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPQYTPNrN2rs",
        "colab_type": "text"
      },
      "source": [
        "Calculate the mean KL divergence between the sum of the uncertainty in q over the whole batch and log probability of r1 at q samples over the whole batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y54qTq2VN2IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    log_r1_q = bimix_gauss.log_prob(q_zxy_samp)   # evaluate the log prob of r1 at the q samples\n",
        "    KL = tf.reduce_mean(log_q_q - log_r1_q)      # average over batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKaMDpMsPL_7",
        "colab_type": "text"
      },
      "source": [
        "Get total loss of network by adding KL divergence (scaled by a ramp factor) to the reconstruction loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG-2nWZvPL2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # THE VICI COST FUNCTION\n",
        "    COST = cost_R + ramp*KL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g241y8kFPbBZ",
        "colab_type": "text"
      },
      "source": [
        "Colate all variables (weights, biases, etc.) of the network into a list. Define an optimizer to use. Initialize and run the whole session. Define function to save network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBHd896AVRs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # VARIABLES LISTS\n",
        "    var_list_VICI = [var for var in tf.trainable_variables() if var.name.startswith(\"VICI\")]\n",
        "        \n",
        "    # DEFINE OPTIMISER (using ADAM here)\n",
        "    optimizer = tf.train.AdamOptimizer(params['initial_training_rate']) \n",
        "    minimize = optimizer.minimize(COST,var_list = var_list_VICI)\n",
        "        \n",
        "    # INITIALISE AND RUN SESSION\n",
        "    init = tf.global_variables_initializer()\n",
        "    session.run(init)\n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIIMnNVpba64",
        "colab_type": "text"
      },
      "source": [
        "Train the network graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNyEwDd7beZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Training Inference Model...')    \n",
        "# START OPTIMISATION OF OELBO\n",
        "indices_generator = batch_manager.SequentialIndexer(params['batch_size'], xsh[0])\n",
        "\n",
        "load_chunk_it = 1\n",
        "# Iterate over requested number of training iterations\n",
        "for i in range(params['num_iterations']):\n",
        "\n",
        "    next_indices = indices_generator.next_indices()\n",
        "\n",
        "    # if load chunks true, load in data by chunks\n",
        "    if params['load_by_chunks'] == True and i == int(params['load_iteration']*load_chunk_it):\n",
        "        x_data, y_data = load_chunk(params['train_set_dir'],params['inf_pars'],params,bounds,fixed_vals)\n",
        "        load_chunk_it += 1\n",
        "\n",
        "    # Make noise realizations and add to training data\n",
        "    next_x_data = x_data[next_indices,:]\n",
        "    if n_conv_r1 != None:\n",
        "        next_y_data = y_data[next_indices,:] + np.random.normal(0,1,size=(params['batch_size'],int(params['ndata']),len(fixed_vals['det'])))\n",
        "    else:\n",
        "        next_y_data = y_data[next_indices,:] + np.random.normal(0,1,size=(params['batch_size'],int(params['ndata']*len(fixed_vals['det']))))\n",
        "    next_y_data /= y_normscale  # required for fast convergence\n",
        "\n",
        "    if params['by_channel'] == False:\n",
        "        next_y_data_new = [] \n",
        "        for sig in next_y_data:\n",
        "            next_y_data_new.append(sig.T)\n",
        "        next_y_data = np.array(next_y_data_new)\n",
        "        del next_y_data_new\n",
        "       \n",
        "    # restore session if wanted\n",
        "    if params['resume_training'] == True and i == 0 :\n",
        "        print(save_dir)\n",
        "        saver.restore(session, save_dir)\n",
        " \n",
        "    # train to minimise the cost function\n",
        "    session.run(minimize, feed_dict={bs_ph:bs, x_ph:next_x_data, y_ph:next_y_data, idx:i})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MWPzKptpv0q",
        "colab_type": "text"
      },
      "source": [
        "## Q network\n",
        "Encoder network which takes as input time series and source parameter truths. Outputs samples from q network latent space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-mhMu_gkOCS",
        "colab_type": "text"
      },
      "source": [
        "### Define initial variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FDpkEKPkNj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "\n",
        "    def __init__(self, name, n_input1=3, n_input2=256, n_output=4, n_weights=2048, n_hlayers=2, drate=0.2, n_filters=8, filter_size=8, maxpool=4, n_conv=2, conv_strides=1, pool_strides=1, num_det=1, batch_norm=False, by_channel=False, weight_init='xavier'):\n",
        "        \n",
        "        self.n_input1 = n_input1                                                # latent space dimension size\n",
        "        self.n_input2 = n_input2                                                # time series size\n",
        "        self.n_output = n_output                                                # number of source parameters to infer\n",
        "        self.n_weights = n_weights                                              # number of fully-connected neurons in each layer\n",
        "        self.n_hlayers = n_hlayers                                              # number of fully-connected layers\n",
        "        self.n_conv = n_conv                                                    # number of convolutional layers\n",
        "        self.drate = drate                                                      # drop out rate in fully-connected layers\n",
        "        self.n_filters = n_filters                                              # number of filters in each convolutional layer\n",
        "        self.filter_size = filter_size                                          # size of filters in each convolutional layer\n",
        "        self.maxpool = maxpool                                                  # maxpooling size in each layer\n",
        "        self.conv_strides = conv_strides                                        # convolutional stride size in each layer\n",
        "        self.pool_strides = pool_strides                                        # max pool stride size in each layer\n",
        "        self.num_det = num_det                                                  # number of detectors used\n",
        "        self.batch_norm = batch_norm                                            # batch normalization on or off in each layer\n",
        "        self.by_channel = by_channel                                            # whether or not to split input up into seperate channels\n",
        "        self.weight_init = weight_init                                          # type of weight initilization to use\n",
        "        network_weights = self._create_weights()                                # initialize the network weights and biases\n",
        "        self.weights = network_weights                                          # store intialized network weights\n",
        "        self.nonlinearity = tf.nn.relu                                          # activation function to use in-between layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gS1euGakVbh",
        "colab_type": "text"
      },
      "source": [
        "### Define network archetecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyCZzCmukVB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _calc_z_mean_and_sigma(self,x,y):\n",
        "        with tf.name_scope(\"VICI_VAE_encoder\"):\n",
        "\n",
        "            # Add convolutional layers if wanted\n",
        "            if self.n_conv is not None:\n",
        "\n",
        "                # Reshape input to a multi-dimensional tensor - single channel\n",
        "                if self.by_channel == True:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, 1, y.shape[1], self.num_det])\n",
        "\n",
        "                    # Append requested number of convolutional layers to network\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool,               # Add convolutional layer\n",
        "                                          self.weights['VICI_VAE_encoder'][weight_name],\n",
        "                                          strides=[1,1,self.conv_strides[i],1],padding='SAME'),\n",
        "                                          self.weights['VICI_VAE_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)                 # Add non-linear activation function\n",
        "\n",
        "                        # Apply batch normalization if wanted\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,\n",
        "                                             tf.Variable(tf.zeros([1,conv_post.shape[2],conv_post.shape[3]], \n",
        "                                             dtype=tf.float32)),tf.Variable(tf.ones([1,conv_post.shape[2],conv_post.shape[3]], \n",
        "                                             dtype=tf.float32)),None,None,0.000001)\n",
        "                            conv_dropout = tf.layers.dropout(conv_batchNorm,    # Apply dropout to layer\n",
        "                                                             rate=self.drate)\n",
        "                        else:\n",
        "                            conv_dropout = tf.layers.dropout(conv_post,         # Apply droptout to layer\n",
        "                                                             rate=self.drate)\n",
        "                        conv_pool = tf.nn.max_pool(conv_dropout,ksize=[1, 1,    # Add max pooling layer\n",
        "                                                   self.maxpool[i], 1],\n",
        "                                                   strides=[1, 1, self.pool_strides[i], 1],\n",
        "                                                   padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([x,tf.reshape(conv_pool, \n",
        "                                                 [-1, int(conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)\n",
        "\n",
        "                # Otherwise, reshape input into 1D tensor - multiple channels \n",
        "                if self.by_channel == False:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, y.shape[1], y.shape[2], 1])\n",
        "\n",
        "                    # Append requested number of convolutional layers to network\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_VAE_encoder'][weight_name],strides=[1,self.conv_strides[i],self.conv_strides[i],1],padding='SAME'),self.weights['VICI_VAE_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                        conv_pool = tf.nn.max_pool(conv_batchNorm,ksize=[1, self.maxpool[i], self.maxpool[i], 1],strides=[1, self.pool_strides[i], self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([x,tf.reshape(conv_pool, [-1, int(conv_pool.shape[1]*conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)\n",
        "\n",
        "            # If no convolutional filters wanted, just use fully-connected layers\n",
        "            else:\n",
        "                fc = tf.concat([x,y],axis=1)\n",
        "\n",
        "            # Append requested number of fully-connected layers\n",
        "            hidden_dropout = fc\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                hidden_pre = tf.add(tf.matmul(hidden_dropout,                   # Add fully connected layer \n",
        "                             self.weights['VICI_VAE_encoder'][weight_name]), \n",
        "                             self.weights['VICI_VAE_encoder'][bias_name])\n",
        "                hidden_post = self.nonlinearity(hidden_pre)                     # Add non-linear activation function\n",
        "\n",
        "                # Add batch normalization if requested by the user\n",
        "                if self.batch_norm == True:\n",
        "                    hidden_batchNorm = tf.nn.batch_normalization(hidden_post,tf.Variable(tf.zeros([hidden_post.shape[1]], dtype=tf.float32)),tf.Variable(tf.ones([hidden_post.shape[1]], dtype=tf.float32)),None,None,0.000001)\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_batchNorm,rate=self.drate)\n",
        "                else:\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_post,rate=self.drate)\n",
        "\n",
        "            # Network returns a mean (loc) and a variance (scale)\n",
        "            loc = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_VAE_encoder']['w_loc']), self.weights['VICI_VAE_encoder']['b_loc'])\n",
        "            scale = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_VAE_encoder']['w_scale']), self.weights['VICI_VAE_encoder']['b_scale'])\n",
        "\n",
        "            tf.summary.histogram('loc', loc)\n",
        "            tf.summary.histogram('scale', scale)\n",
        "            return loc, scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV47BGnko9E",
        "colab_type": "text"
      },
      "source": [
        "### Define function to sample from network predicted Gaussian's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK9FUNL3koxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _sample_from_gaussian_dist(self, num_rows, num_cols, mean, log_sigma_sq):\n",
        "        \"\"\" Function to draw samples from NN predicted Gaussians.\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"sample_in_z_space\"):                                # Just a naming mechanism which labels this operation in the graph\n",
        "            eps = tf.random_normal([num_rows, num_cols], 0, 1.,                 # Define random locations from Gaussian to sample \n",
        "                                   dtype=tf.float32)\n",
        "            sample = tf.add(mean, tf.multiply(tf.sqrt(tf.exp(log_sigma_sq)),    #  \n",
        "                                              eps))\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPkHfH3fkz1z",
        "colab_type": "text"
      },
      "source": [
        "### Define function to initialize weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSGbLjcGrbtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _create_weights(self):\n",
        "        \"\"\" Function to initialize weights of layers\n",
        "        \"\"\"\n",
        "        all_weights = collections.OrderedDict()\n",
        "        with tf.variable_scope(\"VICI_VAE_ENC\"):                                 # Just a naming mechanism (doesn't do much)\n",
        "            # Encoder \n",
        "            all_weights['VICI_VAE_encoder'] = collections.OrderedDict()         # Define dictionary to store network weights\n",
        "            \n",
        "            # Define convolutional weights\n",
        "            if self.n_conv is not None:\n",
        "                dummy = 1\n",
        "                \n",
        "                # Iterate over requested number of convolutional layers\n",
        "                for i in range(self.n_conv):\n",
        "                    weight_name = 'w_conv_' + str(i)\n",
        "                    bias_name = 'b_conv_' + str(i)\n",
        "                    # orthogonal init\n",
        "                    if self.weight_init == 'Orthogonal':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.Orthogonal()\n",
        "                        all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # Variance scaling\n",
        "                    if self.weight_init == 'VarianceScaling':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.VarianceScaling()\n",
        "                        all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # xavier initilization\n",
        "                    if self.weight_init == 'xavier':\n",
        "                        all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(tf.reshape(vae_utils.xavier_init(self.filter_size[i], dummy*self.n_filters[i]),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    all_weights['VICI_VAE_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_filters[i]], dtype=tf.float32))\n",
        "                    tf.summary.histogram(weight_name, all_weights['VICI_VAE_encoder'][weight_name])\n",
        "                    tf.summary.histogram(bias_name, all_weights['VICI_VAE_encoder'][bias_name])\n",
        "                    dummy = self.n_filters[i]\n",
        "\n",
        "                # Determine correct input size to fully-connected layers after having flattened final convolutional layer\n",
        "                total_pool_stride_sum = 0\n",
        "                for j in range(len(self.maxpool)):\n",
        "                    if self.maxpool[j] != 1 and self.pool_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                    else:\n",
        "                        if self.maxpool[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                        if self.pool_strides[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                    if self.conv_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                if self.by_channel == True:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum))\n",
        "                else:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum)*2) \n",
        " \n",
        "            # If no convolutional layers, give input directly to fully-connected layers\n",
        "            else:\n",
        "                fc_input_size = self.n_input1 + self.n_input2\n",
        "\n",
        "            # Iterate over fully-connected layers\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(vae_utils.xavier_init(fc_input_size, self.n_weights[i]), dtype=tf.float32)\n",
        "                all_weights['VICI_VAE_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_weights[i]], dtype=tf.float32))\n",
        "                tf.summary.histogram(weight_name, all_weights['VICI_VAE_encoder'][weight_name])\n",
        "                tf.summary.histogram(bias_name, all_weights['VICI_VAE_encoder'][bias_name])\n",
        "                fc_input_size = self.n_weights[i]\n",
        "                \n",
        "            # Define weights and biases of final layer where mean and variance of Gaussians is predicted\n",
        "            all_weights['VICI_VAE_encoder']['w_loc'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_VAE_encoder']['b_loc'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_loc', all_weights['VICI_VAE_encoder']['w_loc'])\n",
        "            tf.summary.histogram('b_loc', all_weights['VICI_VAE_encoder']['b_loc'])\n",
        "            all_weights['VICI_VAE_encoder']['w_scale'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_VAE_encoder']['b_scale'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_scale', all_weights['VICI_VAE_encoder']['w_scale'])\n",
        "            tf.summary.histogram('b_scale', all_weights['VICI_VAE_encoder']['b_scale'])\n",
        "\n",
        "            all_weights['prior_param'] = collections.OrderedDict()\n",
        "        \n",
        "        return all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJS7h46-p11s",
        "colab_type": "text"
      },
      "source": [
        "## R1 network\n",
        "Encoder network which takes as input time series only. Outputs latent space samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxf8s-12rtp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "\n",
        "    def __init__(self, name, n_input=256, n_output=4, n_weights=2048, n_modes=2, n_hlayers=2, drate=0.2, n_filters=8, filter_size=8, maxpool=4, n_conv=2, conv_strides=1, pool_strides=1, num_det=1, batch_norm=False, by_channel=False, weight_init='xavier'):\n",
        "        \n",
        "        self.n_input = n_input\n",
        "        self.n_output = n_output\n",
        "        self.n_weights = n_weights\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.n_hlayers = n_hlayers\n",
        "        self.n_conv = n_conv\n",
        "        self.n_modes = n_modes\n",
        "        self.drate = drate\n",
        "        self.maxpool = maxpool\n",
        "        self.conv_strides = conv_strides\n",
        "        self.pool_strides = pool_strides\n",
        "        self.num_det = num_det\n",
        "        self.batch_norm = batch_norm\n",
        "        self.by_channel = by_channel\n",
        "        self.weight_init = weight_init\n",
        "\n",
        "        network_weights = self._create_weights()\n",
        "        self.weights = network_weights\n",
        "\n",
        "\n",
        "        self.nonlinearity = tf.nn.relu\n",
        "        self.nonlinearity_mean = tf.clip_by_value\n",
        "\n",
        "    def _calc_z_mean_and_sigma(self,x):\n",
        "        with tf.name_scope(\"VICI_encoder\"):\n",
        " \n",
        "            # Reshape input to a 3D tensor - single channel\n",
        "            if self.n_conv is not None:\n",
        "                if self.by_channel == True:\n",
        "                    conv_pool = tf.reshape(x, shape=[-1, 1, x.shape[1], self.num_det])\n",
        "\n",
        "                    # network for messing with kernel size\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_encoder'][weight_name],strides=[1,1,self.conv_strides[i],1],padding='SAME'),self.weights['VICI_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                            conv_dropout = tf.layers.dropout(conv_batchNorm,rate=self.drate)\n",
        "                        else:\n",
        "                            conv_dropout = tf.layers.dropout(conv_post,rate=self.drate)\n",
        "                        conv_pool = tf.nn.max_pool(conv_dropout,ksize=[1, 1, self.maxpool[i], 1],strides=[1, 1, self.pool_strides[i], 1],padding='SAME')\n",
        "                    \n",
        "                    fc = tf.reshape(conv_pool, [-1, int(conv_pool.shape[2]*conv_pool.shape[3])])\n",
        "                if self.by_channel == False:\n",
        "                    conv_pool = tf.reshape(x, shape=[-1, x.shape[1], x.shape[2], 1])\n",
        "\n",
        "                    # network for messing with kernel size\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_encoder'][weight_name],strides=[1,self.conv_strides[i],self.conv_strides[i],1],padding='SAME'),self.weights['VICI_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                        conv_pool = tf.nn.max_pool(conv_batchNorm,ksize=[1, self.maxpool[i], self.maxpool[i], 1],strides=[1, self.pool_strides[i], self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.reshape(conv_pool, [-1, int(conv_pool.shape[1]*conv_pool.shape[2]*conv_pool.shape[3])])\n",
        "            else:\n",
        "                fc = x\n",
        "\n",
        "            hidden_dropout = fc\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                hidden_pre = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder'][weight_name]), self.weights['VICI_encoder'][bias_name])\n",
        "                hidden_post = self.nonlinearity(hidden_pre)\n",
        "                if self.batch_norm == True:\n",
        "                    hidden_batchNorm = tf.nn.batch_normalization(hidden_post,tf.Variable(tf.zeros([hidden_post.shape[1]], dtype=tf.float32)),tf.Variable(tf.ones([hidden_post.shape[1]], dtype=tf.float32)),None,None,0.000001)\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_batchNorm,rate=self.drate)\n",
        "                else:\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_post,rate=self.drate)\n",
        "            loc = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder']['w_loc']), self.weights['VICI_encoder']['b_loc'])\n",
        "            scale = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder']['w_scale']), self.weights['VICI_encoder']['b_scale'])\n",
        "            weight = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder']['w_weight']), self.weights['VICI_encoder']['b_weight']) \n",
        "\n",
        "            tf.summary.histogram('loc', loc)\n",
        "            tf.summary.histogram('scale', scale)\n",
        "            tf.summary.histogram('weight', weight)\n",
        "            return tf.reshape(loc,(-1,self.n_modes,self.n_output)), tf.reshape(scale,(-1,self.n_modes,self.n_output)), tf.reshape(weight,(-1,self.n_modes))    \n",
        "\n",
        "    def _create_weights(self):\n",
        "        all_weights = collections.OrderedDict()\n",
        "        with tf.variable_scope(\"VICI_ENC\"):            \n",
        "            all_weights['VICI_encoder'] = collections.OrderedDict()\n",
        "\n",
        "            if self.n_conv is not None:\n",
        "                dummy = 1\n",
        "                for i in range(self.n_conv):\n",
        "                    weight_name = 'w_conv_' + str(i)\n",
        "                    bias_name = 'b_conv_' + str(i)\n",
        "                    # orthogonal init\n",
        "                    if self.weight_init == 'Orthogonal':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.Orthogonal()\n",
        "                        all_weights['VICI_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # Variance scaling\n",
        "                    if self.weight_init == 'VarianceScaling':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.VarianceScaling()\n",
        "                        all_weights['VICI_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # xavier initilization\n",
        "                    if self.weight_init == 'xavier':\n",
        "                        all_weights['VICI_encoder'][weight_name] = tf.Variable(tf.reshape(vae_utils.xavier_init(self.filter_size[i], dummy*self.n_filters[i]),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    all_weights['VICI_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_filters[i]], dtype=tf.float32))\n",
        "                    tf.summary.histogram(weight_name, all_weights['VICI_encoder'][weight_name])\n",
        "                    tf.summary.histogram(bias_name, all_weights['VICI_encoder'][bias_name])\n",
        "                    dummy = self.n_filters[i]\n",
        "\n",
        "                total_pool_stride_sum = 0\n",
        "                for j in range(len(self.maxpool)):\n",
        "                    if self.maxpool[j] != 1 and self.pool_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                    else:\n",
        "                        if self.maxpool[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                        if self.pool_strides[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                    if self.conv_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                if self.by_channel == True:\n",
        "                    fc_input_size = int(self.n_input*self.n_filters[i]/(2**total_pool_stride_sum))\n",
        "                else:\n",
        "                    fc_input_size = int((self.n_input*self.n_filters[i]/(2**total_pool_stride_sum)*2))\n",
        "            else:\n",
        "                fc_input_size = self.n_input\n",
        "\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                all_weights['VICI_encoder'][weight_name] = tf.Variable(vae_utils.xavier_init(fc_input_size, self.n_weights[i]), dtype=tf.float32)\n",
        "                all_weights['VICI_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_weights[i]], dtype=tf.float32))\n",
        "                tf.summary.histogram(weight_name, all_weights['VICI_encoder'][weight_name])\n",
        "                tf.summary.histogram(bias_name, all_weights['VICI_encoder'][bias_name])\n",
        "                fc_input_size = self.n_weights[i]\n",
        "            all_weights['VICI_encoder']['w_loc'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output*self.n_modes),dtype=tf.float32)\n",
        "            all_weights['VICI_encoder']['b_loc'] = tf.Variable(tf.zeros([self.n_output*self.n_modes], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_loc', all_weights['VICI_encoder']['w_loc'])\n",
        "            tf.summary.histogram('b_loc', all_weights['VICI_encoder']['b_loc'])\n",
        "            all_weights['VICI_encoder']['w_scale'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output*self.n_modes),dtype=tf.float32)\n",
        "            all_weights['VICI_encoder']['b_scale'] = tf.Variable(tf.zeros([self.n_output*self.n_modes], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_scale', all_weights['VICI_encoder']['w_scale'])\n",
        "            tf.summary.histogram('b_scale', all_weights['VICI_encoder']['b_scale'])\n",
        "            all_weights['VICI_encoder']['w_weight'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_modes),dtype=tf.float32)\n",
        "            all_weights['VICI_encoder']['b_weight'] = tf.Variable(tf.zeros([self.n_modes], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_weight', all_weights['VICI_encoder']['w_weight'])\n",
        "            tf.summary.histogram('b_weight', all_weights['VICI_encoder']['b_weight'])\n",
        "\n",
        "            all_weights['prior_param'] = collections.OrderedDict()\n",
        "        \n",
        "        return all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRrXGxH6p4cp",
        "colab_type": "text"
      },
      "source": [
        "## R2 network\n",
        "Decoder network which takes as input samples from latent space produced by q network. Outputs samples from the posterior when trained properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTjsFdBhr_8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "\n",
        "    def __init__(self, name, wrap_mask, nowrap_mask, n_input1=4, n_input2=256, n_output=3, n_weights=2048, n_hlayers=2, drate=0.2, n_filters=8, filter_size=8, maxpool=4, n_conv=2, conv_strides=1, pool_strides=1, num_det=1, batch_norm=False,by_channel=False, weight_init='xavier'):\n",
        "        \n",
        "        self.n_input1 = n_input1                    # actually the output size\n",
        "        self.n_input2 = n_input2                    # actually the output size\n",
        "        self.n_output = n_output                  # the input data size\n",
        "        self.n_weights = n_weights                # the number of weights were layer\n",
        "        self.n_hlayers = n_hlayers\n",
        "        self.n_conv = n_conv\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.maxpool = maxpool\n",
        "        self.conv_strides = conv_strides\n",
        "        self.pool_strides = pool_strides\n",
        "        self.name = name                          # the name of the network\n",
        "        self.drate = drate                        # dropout rate\n",
        "        self.wrap_mask = wrap_mask                # mask identifying wrapped indices\n",
        "        self.nowrap_mask = nowrap_mask            # mask identifying non-wrapped indices\n",
        "        self.num_det = num_det\n",
        "        self.batch_norm = batch_norm\n",
        "        self.by_channel = by_channel\n",
        "        self.weight_init = weight_init \n",
        "\n",
        "        network_weights = self._create_weights()\n",
        "        self.weights = network_weights\n",
        "\n",
        "        self.nonlinear_loc_nowrap = tf.sigmoid    # activation for non-wrapped location params\n",
        "        self.nonlinear_loc_wrap = tf.sigmoid      # activation for wrapped location params\n",
        "        self.nonlinear_scale_nowrap = tf.identity # activation for non-wrapped scale params\n",
        "        self.nonlinear_scale_wrap = tf.nn.relu    # activation for wrapped scale params  \n",
        "        self.nonlinearity = tf.nn.relu            # activation between hidden layers\n",
        "\n",
        "    def calc_reconstruction(self, z, y):\n",
        "        with tf.name_scope(\"VICI_decoder\"):\n",
        "\n",
        "            # Reshape input to a 3D tensor - single channel\n",
        "            if self.n_conv is not None:\n",
        "                if self.by_channel == True:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, 1, y.shape[1], self.num_det])\n",
        "                    for i in range(self.n_conv):            \n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_decoder'][weight_name],strides=[1,1,self.conv_strides[i],1],padding='SAME'),self.weights['VICI_decoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                            conv_dropout = tf.layers.dropout(conv_batchNorm,rate=self.drate)\n",
        "                        else:\n",
        "                            conv_dropout = tf.layers.dropout(conv_post,rate=self.drate)\n",
        "                        conv_pool = tf.nn.max_pool(conv_dropout,ksize=[1, 1, self.maxpool[i], 1],strides=[1, 1, self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([z,tf.reshape(conv_pool, [-1, int(conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)            \n",
        "                if self.by_channel == False:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, y.shape[1], y.shape[2], 1])\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_decoder'][weight_name],strides=[1,self.conv_strides[i],self.conv_strides[i],1],padding='SAME'),self.weights['VICI_decoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                        conv_pool = tf.nn.max_pool(conv_batchNorm,ksize=[1, self.maxpool[i], self.maxpool[i], 1],strides=[1, self.pool_strides[i], self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([z,tf.reshape(conv_pool, [-1, int(conv_pool.shape[1]*conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)\n",
        "            else:\n",
        "                fc = tf.concat([z,y],axis=1)\n",
        "\n",
        "            hidden_dropout = fc\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                hidden_pre = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_decoder'][weight_name]), self.weights['VICI_decoder'][bias_name])\n",
        "                hidden_post = self.nonlinearity(hidden_pre)\n",
        "                if self.batch_norm == True:\n",
        "                    hidden_batchNorm = tf.nn.batch_normalization(hidden_post,tf.Variable(tf.zeros([hidden_post.shape[1]], dtype=tf.float32)),tf.Variable(tf.ones([hidden_post.shape[1]], dtype=tf.float32)),None,None,0.000001)\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_batchNorm,rate=self.drate)\n",
        "                else:\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_post,rate=self.drate)\n",
        "            loc_all = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_decoder']['w_loc']), self.weights['VICI_decoder']['b_loc'])\n",
        "            scale_all = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_decoder']['w_scale']), self.weights['VICI_decoder']['b_scale'])\n",
        "\n",
        "            # split up the output into non-wrapped and wrapped params and apply appropriate activation\n",
        "            loc_nowrap = self.nonlinear_loc_nowrap(tf.boolean_mask(loc_all,self.nowrap_mask,axis=1))\n",
        "            scale_nowrap = self.nonlinear_scale_nowrap(tf.boolean_mask(scale_all,self.nowrap_mask,axis=1))\n",
        "            if np.sum(self.wrap_mask)>0:\n",
        "                loc_wrap = self.nonlinear_loc_wrap(tf.boolean_mask(loc_all,self.wrap_mask,axis=1))\n",
        "                scale_wrap = -1.0*self.nonlinear_scale_wrap(tf.boolean_mask(scale_all,self.wrap_mask,axis=1))\n",
        "                return loc_nowrap, scale_nowrap, loc_wrap, scale_wrap\n",
        "            else:\n",
        "                return loc_nowrap, scale_nowrap\n",
        "\n",
        "    def _create_weights(self):\n",
        "        all_weights = collections.OrderedDict()\n",
        "\n",
        "        # Decoder\n",
        "        with tf.variable_scope(\"VICI_DEC\"):\n",
        "            all_weights['VICI_decoder'] = collections.OrderedDict()\n",
        "            \n",
        "            if self.n_conv is not None:\n",
        "                dummy = 1\n",
        "                for i in range(self.n_conv):\n",
        "                    weight_name = 'w_conv_' + str(i)\n",
        "                    bias_name = 'b_conv_' + str(i)\n",
        "                    # orthogonal init\n",
        "                    if self.weight_init == 'Orthogonal':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.Orthogonal()\n",
        "                        all_weights['VICI_decoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # Variance scaling\n",
        "                    if self.weight_init == 'VarianceScaling':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.VarianceScaling()\n",
        "                        all_weights['VICI_decoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # xavier initilization\n",
        "                    if self.weight_init == 'xavier':\n",
        "                        all_weights['VICI_decoder'][weight_name] = tf.Variable(tf.reshape(vae_utils.xavier_init(self.filter_size[i], dummy*self.n_filters[i]),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    all_weights['VICI_decoder'][bias_name] = tf.Variable(tf.zeros([self.n_filters[i]], dtype=tf.float32))\n",
        "                    tf.summary.histogram(weight_name, all_weights['VICI_decoder'][weight_name])\n",
        "                    tf.summary.histogram(bias_name, all_weights['VICI_decoder'][bias_name])\n",
        "                    dummy = self.n_filters[i]\n",
        "\n",
        "                total_pool_stride_sum = 0\n",
        "                for j in range(len(self.maxpool)):\n",
        "                    if self.maxpool[j] != 1 and self.pool_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                    else:\n",
        "                        if self.maxpool[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                        if self.pool_strides[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                    if self.conv_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                if self.by_channel == True:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum))\n",
        "                else:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum)*2)\n",
        "            else:\n",
        "                fc_input_size = self.n_input1 + self.n_input2\n",
        "\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                all_weights['VICI_decoder'][weight_name] = tf.Variable(vae_utils.xavier_init(fc_input_size, self.n_weights[i]), dtype=tf.float32)\n",
        "                all_weights['VICI_decoder'][bias_name] = tf.Variable(tf.zeros([self.n_weights[i]], dtype=tf.float32))\n",
        "                tf.summary.histogram(weight_name, all_weights['VICI_decoder'][weight_name])\n",
        "                tf.summary.histogram(bias_name, all_weights['VICI_decoder'][bias_name])\n",
        "                fc_input_size = self.n_weights[i]\n",
        "            all_weights['VICI_decoder']['w_loc'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_decoder']['b_loc'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_loc', all_weights['VICI_decoder']['w_loc'])\n",
        "            tf.summary.histogram('b_loc', all_weights['VICI_decoder']['b_loc'])\n",
        "            all_weights['VICI_decoder']['w_scale'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_decoder']['b_scale'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_scale', all_weights['VICI_decoder']['w_scale'])\n",
        "            tf.summary.histogram('b_scale', all_weights['VICI_decoder']['b_scale'])\n",
        "            \n",
        "            all_weights['prior_param'] = collections.OrderedDict()\n",
        "        \n",
        "        return all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DHQHxuacdFi",
        "colab_type": "text"
      },
      "source": [
        "### Define bounds and default fixed values for source parameters and search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnTS76ZkjxM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source parameter values to use if chosen to be fixed\n",
        "fixed_vals = {'mass_1':50.0,\n",
        "        'mass_2':50.0,\n",
        "        'mc':None,\n",
        "        'geocent_time':0.0,\n",
        "        'phase':0.0,\n",
        "        'ra':1.375,\n",
        "        'dec':-1.2108,\n",
        "        'psi':0.0,\n",
        "        'theta_jn':0.0,\n",
        "        'luminosity_distance':2000.0,\n",
        "        'a_1':0.0,\n",
        "        'a_2':0.0,\n",
        "\t'tilt_1':0.0,\n",
        "\t'tilt_2':0.0,\n",
        "        'phi_12':0.0,\n",
        "        'phi_jl':0.0,\n",
        "        'det':['H1','L1','V1']}                                                 # feel free to edit this if more or less detectors wanted\n",
        "\n",
        "\n",
        "# Prior bounds on source parameters\n",
        "bounds = {'mass_1_min':35.0, 'mass_1_max':80.0,\n",
        "        'mass_2_min':35.0, 'mass_2_max':80.0,\n",
        "        'M_min':70.0, 'M_max':160.0,\n",
        "        'geocent_time_min':0.15,'geocent_time_max':0.35,\n",
        "        'phase_min':0.0, 'phase_max':2.0*np.pi,\n",
        "        'ra_min':0.0, 'ra_max':2.0*np.pi,\n",
        "        'dec_min':-0.5*np.pi, 'dec_max':0.5*np.pi,\n",
        "        'psi_min':0.0, 'psi_max':2.0*np.pi,\n",
        "        'theta_jn_min':0.0, 'theta_jn_max':np.pi,\n",
        "        'a_1_min':0.0, 'a_1_max':0.0,\n",
        "        'a_2_min':0.0, 'a_2_max':0.0,\n",
        "        'tilt_1_min':0.0, 'tilt_1_max':0.0,\n",
        "        'tilt_2_min':0.0, 'tilt_2_max':0.0,\n",
        "        'phi_12_min':0.0, 'phi_12_max':0.0,\n",
        "        'phi_jl_min':0.0, 'phi_jl_max':0.0,\n",
        "        'luminosity_distance_min':1000.0, 'luminosity_distance_max':3000.0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3BnAAf8jyHw",
        "colab_type": "text"
      },
      "source": [
        "### Define other run parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NaP1BzdwW5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################\n",
        "# Main tunable variables\n",
        "##########################\n",
        "ndata = 256                                                                     # sampling frequency\n",
        "rand_pars = ['mass_1','mass_2','luminosity_distance','geocent_time','phase',\n",
        "                 'theta_jn','psi','ra','dec']                                   # parameters to randomize (those not listed here are fixed otherwise)\n",
        "inf_pars=['luminosity_distance','geocent_time','ra','dec'],                     # parameters to infer\n",
        "batch_size = 64                                                                 # Number training samples shown to neural network per iteration\n",
        "weight_init = 'xavier',                                                         #[xavier,VarianceScaling,Orthogonal] # Network model weight initialization    \n",
        "n_modes=7,                                                                      # number of modes in Gaussian mixture model (ideal 7, but may go higher/lower)\n",
        "initial_training_rate=1e-4,                                                     # initial training rate for ADAM optimiser inference model (inverse reconstruction)\n",
        "batch_norm=True,                                                                # if true, do batch normalization in all layers of neural network\n",
        "\n",
        "# FYI, each item in lists below correspond to each layer in networks (i.e. first item first layer)\n",
        "# pool size and pool stride should be same number in each layer\n",
        "n_filters_r1 = [33, 33, 33],                                                    # number of convolutional filters to use in r1 network (must be divisible by 3)\n",
        "n_filters_r2 = [33, 33, 33],                                                    # number of convolutional filters to use in r2 network (must be divisible by 3)\n",
        "n_filters_q = [33, 33, 33],                                                     # number of convolutional filters to use in q network  (must be divisible by 3)\n",
        "filter_size_r1 = [7,7,7],                                                       # size of convolutional fitlers in r1 network\n",
        "filter_size_r2 = [7,7,7],                                                       # size of convolutional filters in r2 network\n",
        "filter_size_q = [7,7,7],                                                        # size of convolutional filters in q network\n",
        "drate = 0.5,                                                                    # dropout rate to use in fully-connected layers\n",
        "maxpool_r1 = [1,2,1],                                                           # size of maxpooling to use in r1 network\n",
        "conv_strides_r1 = [1,1,1],                                                      # size of convolutional stride to use in r1 network\n",
        "pool_strides_r1 = [1,2,1],                                                      # size of max pool stride to use in r1 network\n",
        "maxpool_r2 = [1,2,1],                                                           # size of max pooling to use in r2 network\n",
        "conv_strides_r2 = [1,1,1],                                                      # size of convolutional stride in r2 network\n",
        "pool_strides_r2 = [1,2,1],                                                      # size of max pool stride in r2 network\n",
        "maxpool_q = [1,2,1],                                                            # size of max pooling to use in q network\n",
        "conv_strides_q = [1,1,1],                                                       # size of convolutional stride to use in q network\n",
        "pool_strides_q = [1,2,1],                                                       # size of max pool stride to use in q network\n",
        "n_fc = 512                                                                      # Number of neurons in fully-connected layers\n",
        "z_dimension=8,                                                                  # number of latent space dimensions of model \n",
        "n_weights_r1 = [n_fc,n_fc],                                                     # number fully-connected layers of encoders and decoders in the r1 model (inverse reconstruction)\n",
        "n_weights_r2 = [n_fc,n_fc],                                                     # number fully-connected layers of encoders and decoders in the r2 model (inverse reconstruction)\n",
        "n_weights_q = [n_fc,n_fc],                                                      # number fully-connected layers of encoders and decoders q model\n",
        "##########################\n",
        "# Main tunable variables\n",
        "##########################\n",
        "\n",
        "#############################\n",
        "# optional tunable variables\n",
        "#############################\n",
        "run_label = 'ozgrav-demo_%ddet_%dpar_%dHz_run1' % (len(fixed_vals['det']),len(rand_pars),ndata) # label of run\n",
        "bilby_results_label = 'ozgrav-demo'                                             # label given to bilby results directory\n",
        "r = 2                                                                           # number (to the power of 2) of test samples to use for testing. r = 2 means you want to use 2^2 (i.e 4) test samples\n",
        "pe_test_num = 256                                                               # total number of test samples available to use in directory\n",
        "tot_dataset_size = int(1e4)                                                     # total number of training samples available to use\n",
        "tset_split = int(1e3)                                                           # number of training samples in each training data file\n",
        "save_interval = int(5e4)                                                        # number of iterations to save model and plot validation results corner plots\n",
        "ref_geocent_time=1126259642.5                                                   # reference gps time (not advised to change this)\n",
        "load_chunk_size = 1e4                                                           # Number of training samples to load in at a time.\n",
        "samplers=['vitamin','dynesty'],                                                 # Bayesian samplers to use when comparing ML results (vitamin is ML approach) dynesty,ptemcee,cpnest,emcee\n",
        "\n",
        "# Directory variables\n",
        "plot_dir=\"results/%s\" % run_label,  # output directory to save results plots\n",
        "train_set_dir='training_sets_%ddet_%dpar_%dHz/tset_tot-%d_split-%d' % (len(fixed_vals['det']),len(rand_pars),ndata,tot_dataset_size,tset_split), # location of training set\n",
        "test_set_dir='test_sets/%s/four_parameter_case/test_waveforms' % bilby_results_label,                                                            # location of test set directory waveforms\n",
        "pe_dir='test_sets/%s/four_parameter_case/test' % bilby_results_label,                                                                            # location of test set directory Bayesian PE samples\n",
        "#############################\n",
        "# optional tunable variables\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG3Z6hk2dYA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function for getting list of parameters that need to be fed into the models\n",
        "def get_params():\n",
        "\n",
        "    # Define dictionary to store values used in rest of code \n",
        "    params = dict(\n",
        "        make_corner_plots = True,                                               # if True, make corner plots\n",
        "        make_kl_plot = True,                                                    # If True, go through kl plotting function\n",
        "        make_pp_plot = True,                                                    # If True, go through pp plotting function\n",
        "        make_loss_plot = False,                                                 # If True, generate loss plot from previous plot data\n",
        "        Make_sky_plot=False,                                                    # If True, generate sky plots on corner plots\n",
        "        hyperparam_optim = False,                                               # optimize hyperparameters for model during training using gaussian process minimization\n",
        "        resume_training=False,                                                  # if True, resume training of a model from saved checkpoint\n",
        "        load_by_chunks = True,                                                  # if True, load training samples by a predefined chunk size rather than all at once\n",
        "        ramp = True,                                                            # if true, apply linear ramp to KL loss\n",
        "        print_values=True,                                                      # optionally print loss values every report interval\n",
        "        by_channel = True,                                                      # if True, do convolutions as seperate 1-D channels, if False, stack training samples as 2-D images (n_detectors,(duration*sampling_frequency))\n",
        "        load_plot_data=False,                                                   # Plotting data which has already been generated\n",
        "        doPE = True,                                                            # if True then do bilby PE when generating new testing samples (not advised to change this)\n",
        "        gpu_num=0,                                                              # gpu number run is running on\n",
        "        ndata = ndata,                                                          \n",
        "        run_label=run_label,                                                    \n",
        "        bilby_results_label=bilby_results_label,                                \n",
        "        tot_dataset_size = tot_dataset_size,                                    \n",
        "        tset_split = tset_split,                                                \n",
        "        plot_dir=plot_dir,\n",
        "\n",
        "        # Gaussian Process automated hyperparameter tunning variables\n",
        "        hyperparam_optim_stop = int(1.5e6),                                     # stopping iteration of hyperparameter optimizer per call (ideally 1.5 million) \n",
        "        hyperparam_n_call = 30,                                                 # number of hyperparameter optimization calls (ideally 30)\n",
        "        load_chunk_size = load_chunk_size,                                      \n",
        "        load_iteration = int((load_chunk_size * 25)/batch_size),                # How often to load another chunk of training samples\n",
        "        weight_init = weight_init[0],                                           \n",
        "        n_samples = 1000,                                                       # number of posterior samples to save per reconstruction upon inference (default 3000) \n",
        "        num_iterations=int(1e7)+1,                                              # total number of iterations before ending training of model\n",
        "        initial_training_rate=initial_training_rate[0],                         \n",
        "        batch_size=batch_size,                                                  \n",
        "        batch_norm=batch_norm,                                                  \n",
        "        report_interval=500,                                                    # interval at which to save objective function values and optionally print info during training\n",
        "        n_modes=n_modes[0],                                                     \n",
        "\n",
        "        # FYI, each item in lists below correspond to each layer in networks (i.e. first item first layer)\n",
        "        # pool size and pool stride should be same number in each layer\n",
        "        n_filters_r1 = n_filters_r1[0],                                         \n",
        "        n_filters_r2 = n_filters_r2[0],                                         \n",
        "        n_filters_q = n_filters_q[0],                                           \n",
        "        filter_size_r1 = filter_size_r1[0],                                     \n",
        "        filter_size_r2 = filter_size_r2[0],                                     \n",
        "        filter_size_q = filter_size_q[0],                                       \n",
        "        drate = drate[0],                                                       \n",
        "        maxpool_r1 = maxpool_r1[0],                                             \n",
        "        conv_strides_r1 = conv_strides_r1[0],                                   \n",
        "        pool_strides_r1 = pool_strides_r1[0],                                   \n",
        "        maxpool_r2 = maxpool_r2[0],                                             \n",
        "        conv_strides_r2 = conv_strides_r2[0],                                   \n",
        "        pool_strides_r2 = pool_strides_r2[0],                                   \n",
        "        maxpool_q = maxpool_q[0],                                               \n",
        "        conv_strides_q = conv_strides_q[0],                                     \n",
        "        pool_strides_q = pool_strides_q[0],                                     \n",
        "        ramp_start = 1e4,                                                       # starting iteration of KL divergence ramp (if using)\n",
        "        ramp_end = 1e5,                                                         # ending iteration of KL divergence ramp (if using)\n",
        "        save_interval=save_interval,                                            \n",
        "        plot_interval=save_interval,                                            \n",
        "        z_dimension=z_dimension[0],                                              \n",
        "        n_weights_r1 = n_weights_r1[0],                                         \n",
        "        n_weights_r2 = n_weights_r2[0],                                         \n",
        "        n_weights_q = n_weights_q[0],                                           \n",
        "        duration = 1.0,                                                         # length of training/validation/test sample time series in seconds (haven't tried using at any other value than 1s)\n",
        "        r = r,                                                                  \n",
        "        rand_pars=rand_pars,                                                    \n",
        "        corner_parnames = ['m_{1}\\,(\\mathrm{M}_{\\odot})','m_{2}\\,(\\mathrm{M}_{\\odot})','d_{\\mathrm{L}}\\,(\\mathrm{Mpc})','t_{0}\\,(\\mathrm{seconds})','{\\phi}','\\Theta_{jn}\\,(\\mathrm{rad})','{\\psi}',r'{\\alpha}\\,(\\mathrm{rad})','{\\delta}\\,(\\mathrm{rad})'], # latex source parameter labels for plotting\n",
        "        cornercorner_parnames = ['$m_{1}\\,(\\mathrm{M}_{\\odot})$','$m_{2}\\,(\\mathrm{M}_{\\odot})$','$d_{\\mathrm{L}}\\,(\\mathrm{Mpc})$','$t_{0}\\,(\\mathrm{seconds})$','${\\phi}$','$\\Theta_{jn}\\,(\\mathrm{rad})$','${\\psi}$',r'${\\alpha}\\,(\\mathrm{rad})$','${\\delta}\\,(\\mathrm{rad})$'], # latex source parameter labels for plotting\n",
        "        ref_geocent_time=ref_geocent_time,                                      \n",
        "        training_data_seed=43,                                                  # tensorflow training random seed number\n",
        "        testing_data_seed=44,                                                   # tensorflow testing random seed number\n",
        "        wrap_pars=['phase','psi','ra'],                                         # Parameters to apply Von Mises wrapping on (not advised to change) \n",
        "        inf_pars=inf_pars,                                                      \n",
        "        train_set_dir=train_set_dir,\n",
        "        test_set_dir=test_set_dir,\n",
        "        pe_dir=pe_dir,\n",
        "        KL_cycles = 1,                                                          # number of cycles to repeat for the KL approximation\n",
        "        samplers=samplers,                                                      \n",
        "    )\n",
        "    return params\n",
        "\n",
        "\n",
        "# Save training/test parameters of run\n",
        "params=get_params()\n",
        "params['plot_dir']=params['plot_dir'][0]\n",
        "params['train_set_dir']=params['train_set_dir'][0]\n",
        "params['test_set_dir']=params['test_set_dir'][0]\n",
        "params['pe_dir']=params['pe_dir'][0]\n",
        "params['inf_pars']=params['inf_pars'][0]\n",
        "params['samplers']=params['samplers'][0]\n",
        "f = open(\"params_%s.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(params) )\n",
        "f.close()\n",
        "f = open(\"params_%s_bounds.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(bounds) )\n",
        "f.close()\n",
        "f = open(\"params_%s_fixed_vals.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(fixed_vals) )\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbVDzVuBciLz",
        "colab_type": "text"
      },
      "source": [
        "### Generate Requested number of testing samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1oK3PUnR72q",
        "colab_type": "text"
      },
      "source": [
        "Feel free to use the below command in your own time to generate more testing data. \n",
        "\n",
        "In the paper, we used a total of ~256 test samples. It is recommmened that if you are going to run this many test samples, that you split the test runs over condor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL7wQwyMM3Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --gen_test True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T4N8EJLXgu-",
        "colab_type": "text"
      },
      "source": [
        "# Further Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maLy6ASgXkQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}