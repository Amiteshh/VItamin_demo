{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OzGrav_VItamin_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hagabbar/OzGrav_demo/blob/master/OzGrav_VItamin_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzz4HG9RKqfV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "4e27cc6a-d3fb-4e16-e889-716c7bda868f"
      },
      "source": [
        "!git pull "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)   \rUnpacking objects:  66% (2/3)   \rUnpacking objects: 100% (3/3)   \rUnpacking objects: 100% (3/3), done.\n",
            "From https://github.com/hagabbar/OzGrav_demo\n",
            "   a82ae1b..c323a78  master     -> origin/master\n",
            "Updating a82ae1b..c323a78\n",
            "Fast-forward\n",
            " VICI_code_usage_example.py | 12 \u001b[32m+++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 11 insertions(+), 1 deletion(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdoMTkVPVL6E",
        "colab_type": "text"
      },
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHVnnfXANukP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "ac16c9a6-bd7f-4c26-eb52-c498178834f8"
      },
      "source": [
        "!git clone https://github.com/hagabbar/OzGrav_demo.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OzGrav_demo'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 79 (delta 21), reused 57 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tA7BWiUN1NK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "d6475651-8137-4da9-81ab-764f0f225acf"
      },
      "source": [
        "%cd OzGrav_demo/\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/OzGrav_demo\n",
            "bilby_pe.py         OzGrav_VItamin_demo.ipynb  \u001b[0m\u001b[01;34mtest_sets\u001b[0m/\n",
            "LICENSE             plotsky.py                 \u001b[01;34mtraining_sets_3det_9par_256Hz\u001b[0m/\n",
            "make_test_plots.py  plots.py                   VICI_code_usage_example.py\n",
            "\u001b[01;34mModels\u001b[0m/             README.md\n",
            "\u001b[01;34mNeural_Networks\u001b[0m/    requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI04ILUsRgRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naVHRD3ugIol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZdzHYaSVPvb",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "This tutorial is based on the paper Bayesian Parameter Estimation using Conditional Variational Autoencoders for Gravitational-wave Astronomy (https://arxiv.org/abs/1909.06296). In this demo, we will show how a form of conditional variational autoencoders work and how to reproduce Bayesian posteriors several orders of magnitude faster than exisitng Bayesian techniques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvTJ5Js5ouz0",
        "colab_type": "text"
      },
      "source": [
        "## Network Training Portion of VItamin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBHd896AVRs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "session = tf.Session(graph=graph)\n",
        "with graph.as_default():\n",
        "\n",
        "    # PLACE HOLDERS\n",
        "    bs_ph = tf.placeholder(dtype=tf.int64, name=\"bs_ph\")                       # batch size placeholder\n",
        "    x_ph = tf.placeholder(dtype=tf.float32, shape=[None, xsh[1]], name=\"x_ph\") # params placeholder\n",
        "    if n_conv_r1 != None:\n",
        "        if params['by_channel'] == True:\n",
        "            y_ph = tf.placeholder(dtype=tf.float32, shape=[None,ysh,len(fixed_vals['det'])], name=\"y_ph\")    # data placeholder\n",
        "        else:\n",
        "            y_ph = tf.placeholder(dtype=tf.float32, shape=[None,len(fixed_vals['det']),ysh], name=\"y_ph\")\n",
        "    else:\n",
        "        y_ph = tf.placeholder(dtype=tf.float32, shape=[None,ysh], name=\"y_ph\")    # data placeholder\n",
        "    idx = tf.placeholder(tf.int32)\n",
        "\n",
        "    # LOAD VICI NEURAL NETWORKS\n",
        "    r2_xzy = VICI_decoder.VariationalAutoencoder('VICI_decoder', wrap_mask, nowrap_mask, \n",
        "                                                  n_input1=z_dimension, n_input2=ysh_conv_r2, n_output=xsh[1], \n",
        "                                                  n_weights=n_weights_r2, n_hlayers=n_hlayers_r2, \n",
        "                                                  drate=drate, n_filters=n_filters_r2, filter_size=filter_size_r2,\n",
        "                                                  maxpool=maxpool_r2, n_conv=n_conv_r2, conv_strides=conv_strides_r2, pool_strides=pool_strides_r2, num_det=num_det, batch_norm=batch_norm, by_channel=params['by_channel'], weight_init=params['weight_init'])\n",
        "    r1_zy = VICI_encoder.VariationalAutoencoder('VICI_encoder', n_input=ysh_conv_r1, n_output=z_dimension, \n",
        "                                                  n_weights=n_weights_r1, n_modes=n_modes, \n",
        "                                                  n_hlayers=n_hlayers_r1, drate=drate, n_filters=n_filters_r1, \n",
        "                                                  filter_size=filter_size_r1,maxpool=maxpool_r1, n_conv=n_conv_r1, conv_strides=conv_strides_r1, pool_strides=pool_strides_r1, num_det=num_det, batch_norm=batch_norm, by_channel=params['by_channel'], weight_init=params['weight_init'])\n",
        "    q_zxy = VICI_VAE_encoder.VariationalAutoencoder('VICI_VAE_encoder', n_input1=xsh[1], n_input2=ysh_conv_q, \n",
        "                                                    n_output=z_dimension, n_weights=n_weights_q, \n",
        "                                                    n_hlayers=n_hlayers_q, drate=drate, n_filters=n_filters_q, \n",
        "                                                    filter_size=filter_size_q,maxpool=maxpool_q, n_conv=n_conv_q, conv_strides=conv_strides_q, pool_strides=pool_strides_q, num_det=num_det, batch_norm=batch_norm, by_channel=params['by_channel'], weight_init=params['weight_init']) # used to sample from q(z|x,y)?\n",
        "    tf.set_random_seed(np.random.randint(0,10))\n",
        "\n",
        "    ramp = (tf.log(tf.dtypes.cast(idx,dtype=tf.float32)) - tf.log(ramp_start))/(tf.log(ramp_end)-tf.log(ramp_start))\n",
        "    ramp = tf.minimum(tf.math.maximum(0.0,ramp),1.0)\n",
        "        \n",
        "    if params['ramp'] == False:\n",
        "        ramp = 1.0\n",
        " \n",
        "    # reduce the y data size\n",
        "    y_conv = y_ph\n",
        "\n",
        "    # GET r1(z|y)\n",
        "    # run inverse autoencoder to generate mean and logvar of z given y data - these are the parameters for r1(z|y)\n",
        "    r1_loc, r1_scale, r1_weight = r1_zy._calc_z_mean_and_sigma(y_conv)\n",
        "    r1_scale = tf.sqrt(SMALL_CONSTANT + tf.exp(r1_scale))\n",
        "    # get l1 loss term\n",
        "    l1_loss_weight = ramp*1e-3*tf.reduce_sum(tf.math.abs(r1_weight),1)\n",
        "    r1_weight = ramp*tf.squeeze(r1_weight)\n",
        "        \n",
        "    # define the r1(z|y) mixture model\n",
        "    bimix_gauss = tfd.MixtureSameFamily(\n",
        "                      mixture_distribution=tfd.Categorical(logits=ramp*r1_weight),\n",
        "                      components_distribution=tfd.MultivariateNormalDiag(\n",
        "                      loc=r1_loc,\n",
        "                      scale_diag=r1_scale))\n",
        "\n",
        "\n",
        "    # DRAW FROM r1(z|y) - given the Gaussian parameters generate z samples\n",
        "    r1_zy_samp = bimix_gauss.sample()        \n",
        "        \n",
        "    # GET q(z|x,y)\n",
        "    q_zxy_mean, q_zxy_log_sig_sq = q_zxy._calc_z_mean_and_sigma(x_ph,y_conv)\n",
        "\n",
        "    # DRAW FROM q(z|x,y)\n",
        "    q_zxy_samp = q_zxy._sample_from_gaussian_dist(bs_ph, z_dimension, q_zxy_mean, tf.log(SMALL_CONSTANT + tf.exp(q_zxy_log_sig_sq)))\n",
        "        \n",
        "    # GET r2(x|z,y)\n",
        "    reconstruction_xzy = r2_xzy.calc_reconstruction(q_zxy_samp,y_conv)\n",
        "    r2_xzy_mean_nowrap = reconstruction_xzy[0]\n",
        "    r2_xzy_log_sig_sq_nowrap = reconstruction_xzy[1]\n",
        "    if np.sum(wrap_mask)>0:\n",
        "        r2_xzy_mean_wrap = reconstruction_xzy[2]\n",
        "        r2_xzy_log_sig_sq_wrap = reconstruction_xzy[3]\n",
        "\n",
        "    # COST FROM RECONSTRUCTION - Gaussian parts\n",
        "    normalising_factor_x = -0.5*tf.log(SMALL_CONSTANT + tf.exp(r2_xzy_log_sig_sq_nowrap)) - 0.5*np.log(2.0*np.pi)   # -0.5*log(sig^2) - 0.5*log(2*pi)\n",
        "    square_diff_between_mu_and_x = tf.square(r2_xzy_mean_nowrap - tf.boolean_mask(x_ph,nowrap_mask,axis=1))         # (mu - x)^2\n",
        "\n",
        "    inside_exp_x = -0.5 * tf.divide(square_diff_between_mu_and_x,SMALL_CONSTANT + tf.exp(r2_xzy_log_sig_sq_nowrap)) # -0.5*(mu - x)^2 / sig^2\n",
        "    reconstr_loss_x = tf.reduce_sum(normalising_factor_x + inside_exp_x,axis=1,keepdims=True)                       # sum_dim(-0.5*log(sig^2) - 0.5*log(2*pi) - 0.5*(mu - x)^2 / sig^2)\n",
        "\n",
        "    # COST FROM RECONSTRUCTION - Von Mises parts\n",
        "    if np.sum(wrap_mask)>0:\n",
        "        con = tf.reshape(tf.math.reciprocal(SMALL_CONSTANT + tf.exp(r2_xzy_log_sig_sq_wrap)),[-1,wrap_len])   # modelling wrapped scale output as log variance\n",
        "        von_mises = tfp.distributions.VonMises(loc=2.0*np.pi*(tf.reshape(r2_xzy_mean_wrap,[-1,wrap_len])-0.5), concentration=con)   # define p_vm(2*pi*mu,con=1/sig^2)\n",
        "        reconstr_loss_vm = tf.reduce_sum(von_mises.log_prob(2.0*np.pi*(tf.reshape(tf.boolean_mask(x_ph,wrap_mask,axis=1),[-1,wrap_len]) - 0.5)),axis=1)   # 2pi is the von mises input range\n",
        "        cost_R = -1.0*tf.reduce_mean(reconstr_loss_x + reconstr_loss_vm) # average over batch\n",
        "        r2_xzy_mean = tf.gather(tf.concat([r2_xzy_mean_nowrap,r2_xzy_mean_wrap],axis=1),tf.constant(idx_mask),axis=1)\n",
        "        r2_xzy_scale = tf.gather(tf.concat([r2_xzy_log_sig_sq_nowrap,r2_xzy_log_sig_sq_wrap],axis=1),tf.constant(idx_mask),axis=1) \n",
        "    else:\n",
        "        cost_R = -1.0*tf.reduce_mean(reconstr_loss_x)    \n",
        "        r2_xzy_mean = r2_xzy_mean_nowrap\n",
        "        r2_xzy_scale = r2_xzy_log_sig_sq_nowrap\n",
        "\n",
        "    # compute montecarlo KL - first compute the analytic self entropy of q \n",
        "    normalising_factor_kl = -0.5*tf.log(SMALL_CONSTANT + tf.exp(q_zxy_log_sig_sq)) - 0.5*np.log(2.0*np.pi)   # -0.5*log(sig^2) - 0.5*log(2*pi)\n",
        "    square_diff_between_qz_and_q = tf.square(q_zxy_mean - q_zxy_samp)                                        # (mu - x)^2\n",
        "    inside_exp_q = -0.5 * tf.divide(square_diff_between_qz_and_q,SMALL_CONSTANT + tf.exp(q_zxy_log_sig_sq))  # -0.5*(mu - x)^2 / sig^2\n",
        "    log_q_q = tf.reduce_sum(normalising_factor_kl + inside_exp_q,axis=1,keepdims=True)                       # sum_dim(-0.5*log(sig^2) - 0.5*log(2*pi) - 0.5*(mu - x)^2 / sig^2)\n",
        "    log_r1_q = bimix_gauss.log_prob(q_zxy_samp)   # evaluate the log prob of r1 at the q samples\n",
        "    KL = tf.reduce_mean(log_q_q - log_r1_q)      # average over batch\n",
        "\n",
        "    # THE VICI COST FUNCTION\n",
        "    COST = cost_R + ramp*KL\n",
        "\n",
        "    # VARIABLES LISTS\n",
        "    var_list_VICI = [var for var in tf.trainable_variables() if var.name.startswith(\"VICI\")]\n",
        "        \n",
        "    # DEFINE OPTIMISER (using ADAM here)\n",
        "    optimizer = tf.train.AdamOptimizer(params['initial_training_rate']) \n",
        "    minimize = optimizer.minimize(COST,var_list = var_list_VICI)\n",
        "        \n",
        "    # INITIALISE AND RUN SESSION\n",
        "    init = tf.global_variables_initializer()\n",
        "    session.run(init)\n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MWPzKptpv0q",
        "colab_type": "text"
      },
      "source": [
        "## Q network\n",
        "Encoder network which takes as input time series and source parameter truths. Outputs samples from the latent space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSGbLjcGrbtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "\n",
        "    def __init__(self, name, n_input1=3, n_input2=256, n_output=4, n_weights=2048, n_hlayers=2, drate=0.2, n_filters=8, filter_size=8, maxpool=4, n_conv=2, conv_strides=1, pool_strides=1, num_det=1, batch_norm=False, by_channel=False, weight_init='xavier'):\n",
        "        \n",
        "        self.n_input1 = n_input1\n",
        "        self.n_input2 = n_input2\n",
        "        self.n_output = n_output\n",
        "        self.n_weights = n_weights\n",
        "\n",
        "        self.n_hlayers = n_hlayers\n",
        "        self.n_conv = n_conv\n",
        "        self.drate = drate\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.maxpool = maxpool\n",
        "        self.conv_strides = conv_strides\n",
        "        self.pool_strides = pool_strides\n",
        "        self.num_det = num_det\n",
        "        self.batch_norm = batch_norm\n",
        "        self.by_channel = by_channel\n",
        "        self.weight_init = weight_init\n",
        "\n",
        "        network_weights = self._create_weights()\n",
        "        self.weights = network_weights\n",
        "        self.nonlinearity = tf.nn.relu\n",
        "\n",
        "    def _calc_z_mean_and_sigma(self,x,y):\n",
        "        with tf.name_scope(\"VICI_VAE_encoder\"):\n",
        "\n",
        "            # Reshape input to a 3D tensor - single channel\n",
        "            if self.n_conv is not None:\n",
        "                if self.by_channel == True:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, 1, y.shape[1], self.num_det])\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_VAE_encoder'][weight_name],strides=[1,1,self.conv_strides[i],1],padding='SAME'),self.weights['VICI_VAE_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                            conv_dropout = tf.layers.dropout(conv_batchNorm,rate=self.drate)\n",
        "                        else:\n",
        "                            conv_dropout = tf.layers.dropout(conv_post,rate=self.drate)\n",
        "                        conv_pool = tf.nn.max_pool(conv_dropout,ksize=[1, 1, self.maxpool[i], 1],strides=[1, 1, self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([x,tf.reshape(conv_pool, [-1, int(conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)\n",
        "                if self.by_channel == False:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, y.shape[1], y.shape[2], 1])\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_VAE_encoder'][weight_name],strides=[1,self.conv_strides[i],self.conv_strides[i],1],padding='SAME'),self.weights['VICI_VAE_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                        conv_pool = tf.nn.max_pool(conv_batchNorm,ksize=[1, self.maxpool[i], self.maxpool[i], 1],strides=[1, self.pool_strides[i], self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([x,tf.reshape(conv_pool, [-1, int(conv_pool.shape[1]*conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)\n",
        "            else:\n",
        "                fc = tf.concat([x,y],axis=1)\n",
        "\n",
        "            hidden_dropout = fc\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                hidden_pre = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_VAE_encoder'][weight_name]), self.weights['VICI_VAE_encoder'][bias_name])\n",
        "                hidden_post = self.nonlinearity(hidden_pre)\n",
        "                if self.batch_norm == True:\n",
        "                    hidden_batchNorm = tf.nn.batch_normalization(hidden_post,tf.Variable(tf.zeros([hidden_post.shape[1]], dtype=tf.float32)),tf.Variable(tf.ones([hidden_post.shape[1]], dtype=tf.float32)),None,None,0.000001)\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_batchNorm,rate=self.drate)\n",
        "                else:\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_post,rate=self.drate)\n",
        "            loc = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_VAE_encoder']['w_loc']), self.weights['VICI_VAE_encoder']['b_loc'])\n",
        "            scale = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_VAE_encoder']['w_scale']), self.weights['VICI_VAE_encoder']['b_scale'])\n",
        "\n",
        "            tf.summary.histogram('loc', loc)\n",
        "            tf.summary.histogram('scale', scale)\n",
        "            return loc, scale\n",
        "\n",
        "\n",
        "    def _sample_from_gaussian_dist(self, num_rows, num_cols, mean, log_sigma_sq):\n",
        "        with tf.name_scope(\"sample_in_z_space\"):\n",
        "            eps = tf.random_normal([num_rows, num_cols], 0, 1., dtype=tf.float32)\n",
        "            sample = tf.add(mean, tf.multiply(tf.sqrt(tf.exp(log_sigma_sq)), eps))\n",
        "        return sample\n",
        "\n",
        "    def _create_weights(self):\n",
        "        all_weights = collections.OrderedDict()\n",
        "        with tf.variable_scope(\"VICI_VAE_ENC\"):\n",
        "            # Encoder\n",
        "            all_weights['VICI_VAE_encoder'] = collections.OrderedDict()\n",
        "            \n",
        "            if self.n_conv is not None:\n",
        "                dummy = 1\n",
        "                for i in range(self.n_conv):\n",
        "                    weight_name = 'w_conv_' + str(i)\n",
        "                    bias_name = 'b_conv_' + str(i)\n",
        "                    # orthogonal init\n",
        "                    if self.weight_init == 'Orthogonal':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.Orthogonal()\n",
        "                        all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # Variance scaling\n",
        "                    if self.weight_init == 'VarianceScaling':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.VarianceScaling()\n",
        "                        all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # xavier initilization\n",
        "                    if self.weight_init == 'xavier':\n",
        "                        all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(tf.reshape(vae_utils.xavier_init(self.filter_size[i], dummy*self.n_filters[i]),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    all_weights['VICI_VAE_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_filters[i]], dtype=tf.float32))\n",
        "                    tf.summary.histogram(weight_name, all_weights['VICI_VAE_encoder'][weight_name])\n",
        "                    tf.summary.histogram(bias_name, all_weights['VICI_VAE_encoder'][bias_name])\n",
        "                    dummy = self.n_filters[i]\n",
        "\n",
        "                total_pool_stride_sum = 0\n",
        "                for j in range(len(self.maxpool)):\n",
        "                    if self.maxpool[j] != 1 and self.pool_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                    else:\n",
        "                        if self.maxpool[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                        if self.pool_strides[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                    if self.conv_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                if self.by_channel == True:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum))\n",
        "                else:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum)*2) \n",
        "            else:\n",
        "                fc_input_size = self.n_input1 + self.n_input2\n",
        "\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                all_weights['VICI_VAE_encoder'][weight_name] = tf.Variable(vae_utils.xavier_init(fc_input_size, self.n_weights[i]), dtype=tf.float32)\n",
        "                all_weights['VICI_VAE_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_weights[i]], dtype=tf.float32))\n",
        "                tf.summary.histogram(weight_name, all_weights['VICI_VAE_encoder'][weight_name])\n",
        "                tf.summary.histogram(bias_name, all_weights['VICI_VAE_encoder'][bias_name])\n",
        "                fc_input_size = self.n_weights[i]\n",
        "            all_weights['VICI_VAE_encoder']['w_loc'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_VAE_encoder']['b_loc'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_loc', all_weights['VICI_VAE_encoder']['w_loc'])\n",
        "            tf.summary.histogram('b_loc', all_weights['VICI_VAE_encoder']['b_loc'])\n",
        "            all_weights['VICI_VAE_encoder']['w_scale'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_VAE_encoder']['b_scale'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_scale', all_weights['VICI_VAE_encoder']['w_scale'])\n",
        "            tf.summary.histogram('b_scale', all_weights['VICI_VAE_encoder']['b_scale'])\n",
        "\n",
        "            all_weights['prior_param'] = collections.OrderedDict()\n",
        "        \n",
        "        return all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJS7h46-p11s",
        "colab_type": "text"
      },
      "source": [
        "## R1 network\n",
        "Encoder network which takes as input time series only. Outputs latent space samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxf8s-12rtp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "\n",
        "    def __init__(self, name, n_input=256, n_output=4, n_weights=2048, n_modes=2, n_hlayers=2, drate=0.2, n_filters=8, filter_size=8, maxpool=4, n_conv=2, conv_strides=1, pool_strides=1, num_det=1, batch_norm=False, by_channel=False, weight_init='xavier'):\n",
        "        \n",
        "        self.n_input = n_input\n",
        "        self.n_output = n_output\n",
        "        self.n_weights = n_weights\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.n_hlayers = n_hlayers\n",
        "        self.n_conv = n_conv\n",
        "        self.n_modes = n_modes\n",
        "        self.drate = drate\n",
        "        self.maxpool = maxpool\n",
        "        self.conv_strides = conv_strides\n",
        "        self.pool_strides = pool_strides\n",
        "        self.num_det = num_det\n",
        "        self.batch_norm = batch_norm\n",
        "        self.by_channel = by_channel\n",
        "        self.weight_init = weight_init\n",
        "\n",
        "        network_weights = self._create_weights()\n",
        "        self.weights = network_weights\n",
        "\n",
        "\n",
        "        self.nonlinearity = tf.nn.relu\n",
        "        self.nonlinearity_mean = tf.clip_by_value\n",
        "\n",
        "    def _calc_z_mean_and_sigma(self,x):\n",
        "        with tf.name_scope(\"VICI_encoder\"):\n",
        " \n",
        "            # Reshape input to a 3D tensor - single channel\n",
        "            if self.n_conv is not None:\n",
        "                if self.by_channel == True:\n",
        "                    conv_pool = tf.reshape(x, shape=[-1, 1, x.shape[1], self.num_det])\n",
        "\n",
        "                    # network for messing with kernel size\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_encoder'][weight_name],strides=[1,1,self.conv_strides[i],1],padding='SAME'),self.weights['VICI_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                            conv_dropout = tf.layers.dropout(conv_batchNorm,rate=self.drate)\n",
        "                        else:\n",
        "                            conv_dropout = tf.layers.dropout(conv_post,rate=self.drate)\n",
        "                        conv_pool = tf.nn.max_pool(conv_dropout,ksize=[1, 1, self.maxpool[i], 1],strides=[1, 1, self.pool_strides[i], 1],padding='SAME')\n",
        "                    \n",
        "                    fc = tf.reshape(conv_pool, [-1, int(conv_pool.shape[2]*conv_pool.shape[3])])\n",
        "                if self.by_channel == False:\n",
        "                    conv_pool = tf.reshape(x, shape=[-1, x.shape[1], x.shape[2], 1])\n",
        "\n",
        "                    # network for messing with kernel size\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_encoder'][weight_name],strides=[1,self.conv_strides[i],self.conv_strides[i],1],padding='SAME'),self.weights['VICI_encoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                        conv_pool = tf.nn.max_pool(conv_batchNorm,ksize=[1, self.maxpool[i], self.maxpool[i], 1],strides=[1, self.pool_strides[i], self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.reshape(conv_pool, [-1, int(conv_pool.shape[1]*conv_pool.shape[2]*conv_pool.shape[3])])\n",
        "            else:\n",
        "                fc = x\n",
        "\n",
        "            hidden_dropout = fc\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                hidden_pre = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder'][weight_name]), self.weights['VICI_encoder'][bias_name])\n",
        "                hidden_post = self.nonlinearity(hidden_pre)\n",
        "                if self.batch_norm == True:\n",
        "                    hidden_batchNorm = tf.nn.batch_normalization(hidden_post,tf.Variable(tf.zeros([hidden_post.shape[1]], dtype=tf.float32)),tf.Variable(tf.ones([hidden_post.shape[1]], dtype=tf.float32)),None,None,0.000001)\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_batchNorm,rate=self.drate)\n",
        "                else:\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_post,rate=self.drate)\n",
        "            loc = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder']['w_loc']), self.weights['VICI_encoder']['b_loc'])\n",
        "            scale = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder']['w_scale']), self.weights['VICI_encoder']['b_scale'])\n",
        "            weight = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_encoder']['w_weight']), self.weights['VICI_encoder']['b_weight']) \n",
        "\n",
        "            tf.summary.histogram('loc', loc)\n",
        "            tf.summary.histogram('scale', scale)\n",
        "            tf.summary.histogram('weight', weight)\n",
        "            return tf.reshape(loc,(-1,self.n_modes,self.n_output)), tf.reshape(scale,(-1,self.n_modes,self.n_output)), tf.reshape(weight,(-1,self.n_modes))    \n",
        "\n",
        "    def _create_weights(self):\n",
        "        all_weights = collections.OrderedDict()\n",
        "        with tf.variable_scope(\"VICI_ENC\"):            \n",
        "            all_weights['VICI_encoder'] = collections.OrderedDict()\n",
        "\n",
        "            if self.n_conv is not None:\n",
        "                dummy = 1\n",
        "                for i in range(self.n_conv):\n",
        "                    weight_name = 'w_conv_' + str(i)\n",
        "                    bias_name = 'b_conv_' + str(i)\n",
        "                    # orthogonal init\n",
        "                    if self.weight_init == 'Orthogonal':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.Orthogonal()\n",
        "                        all_weights['VICI_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # Variance scaling\n",
        "                    if self.weight_init == 'VarianceScaling':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.VarianceScaling()\n",
        "                        all_weights['VICI_encoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # xavier initilization\n",
        "                    if self.weight_init == 'xavier':\n",
        "                        all_weights['VICI_encoder'][weight_name] = tf.Variable(tf.reshape(vae_utils.xavier_init(self.filter_size[i], dummy*self.n_filters[i]),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    all_weights['VICI_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_filters[i]], dtype=tf.float32))\n",
        "                    tf.summary.histogram(weight_name, all_weights['VICI_encoder'][weight_name])\n",
        "                    tf.summary.histogram(bias_name, all_weights['VICI_encoder'][bias_name])\n",
        "                    dummy = self.n_filters[i]\n",
        "\n",
        "                total_pool_stride_sum = 0\n",
        "                for j in range(len(self.maxpool)):\n",
        "                    if self.maxpool[j] != 1 and self.pool_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                    else:\n",
        "                        if self.maxpool[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                        if self.pool_strides[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                    if self.conv_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                if self.by_channel == True:\n",
        "                    fc_input_size = int(self.n_input*self.n_filters[i]/(2**total_pool_stride_sum))\n",
        "                else:\n",
        "                    fc_input_size = int((self.n_input*self.n_filters[i]/(2**total_pool_stride_sum)*2))\n",
        "            else:\n",
        "                fc_input_size = self.n_input\n",
        "\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                all_weights['VICI_encoder'][weight_name] = tf.Variable(vae_utils.xavier_init(fc_input_size, self.n_weights[i]), dtype=tf.float32)\n",
        "                all_weights['VICI_encoder'][bias_name] = tf.Variable(tf.zeros([self.n_weights[i]], dtype=tf.float32))\n",
        "                tf.summary.histogram(weight_name, all_weights['VICI_encoder'][weight_name])\n",
        "                tf.summary.histogram(bias_name, all_weights['VICI_encoder'][bias_name])\n",
        "                fc_input_size = self.n_weights[i]\n",
        "            all_weights['VICI_encoder']['w_loc'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output*self.n_modes),dtype=tf.float32)\n",
        "            all_weights['VICI_encoder']['b_loc'] = tf.Variable(tf.zeros([self.n_output*self.n_modes], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_loc', all_weights['VICI_encoder']['w_loc'])\n",
        "            tf.summary.histogram('b_loc', all_weights['VICI_encoder']['b_loc'])\n",
        "            all_weights['VICI_encoder']['w_scale'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output*self.n_modes),dtype=tf.float32)\n",
        "            all_weights['VICI_encoder']['b_scale'] = tf.Variable(tf.zeros([self.n_output*self.n_modes], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_scale', all_weights['VICI_encoder']['w_scale'])\n",
        "            tf.summary.histogram('b_scale', all_weights['VICI_encoder']['b_scale'])\n",
        "            all_weights['VICI_encoder']['w_weight'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_modes),dtype=tf.float32)\n",
        "            all_weights['VICI_encoder']['b_weight'] = tf.Variable(tf.zeros([self.n_modes], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_weight', all_weights['VICI_encoder']['w_weight'])\n",
        "            tf.summary.histogram('b_weight', all_weights['VICI_encoder']['b_weight'])\n",
        "\n",
        "            all_weights['prior_param'] = collections.OrderedDict()\n",
        "        \n",
        "        return all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRrXGxH6p4cp",
        "colab_type": "text"
      },
      "source": [
        "## R2 network\n",
        "Decoder network which takes as input samples from latent space produced by q network. Outputs samples from the posterior when trained properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTjsFdBhr_8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "\n",
        "    def __init__(self, name, wrap_mask, nowrap_mask, n_input1=4, n_input2=256, n_output=3, n_weights=2048, n_hlayers=2, drate=0.2, n_filters=8, filter_size=8, maxpool=4, n_conv=2, conv_strides=1, pool_strides=1, num_det=1, batch_norm=False,by_channel=False, weight_init='xavier'):\n",
        "        \n",
        "        self.n_input1 = n_input1                    # actually the output size\n",
        "        self.n_input2 = n_input2                    # actually the output size\n",
        "        self.n_output = n_output                  # the input data size\n",
        "        self.n_weights = n_weights                # the number of weights were layer\n",
        "        self.n_hlayers = n_hlayers\n",
        "        self.n_conv = n_conv\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.maxpool = maxpool\n",
        "        self.conv_strides = conv_strides\n",
        "        self.pool_strides = pool_strides\n",
        "        self.name = name                          # the name of the network\n",
        "        self.drate = drate                        # dropout rate\n",
        "        self.wrap_mask = wrap_mask                # mask identifying wrapped indices\n",
        "        self.nowrap_mask = nowrap_mask            # mask identifying non-wrapped indices\n",
        "        self.num_det = num_det\n",
        "        self.batch_norm = batch_norm\n",
        "        self.by_channel = by_channel\n",
        "        self.weight_init = weight_init \n",
        "\n",
        "        network_weights = self._create_weights()\n",
        "        self.weights = network_weights\n",
        "\n",
        "        self.nonlinear_loc_nowrap = tf.sigmoid    # activation for non-wrapped location params\n",
        "        self.nonlinear_loc_wrap = tf.sigmoid      # activation for wrapped location params\n",
        "        self.nonlinear_scale_nowrap = tf.identity # activation for non-wrapped scale params\n",
        "        self.nonlinear_scale_wrap = tf.nn.relu    # activation for wrapped scale params  \n",
        "        self.nonlinearity = tf.nn.relu            # activation between hidden layers\n",
        "\n",
        "    def calc_reconstruction(self, z, y):\n",
        "        with tf.name_scope(\"VICI_decoder\"):\n",
        "\n",
        "            # Reshape input to a 3D tensor - single channel\n",
        "            if self.n_conv is not None:\n",
        "                if self.by_channel == True:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, 1, y.shape[1], self.num_det])\n",
        "                    for i in range(self.n_conv):            \n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_decoder'][weight_name],strides=[1,1,self.conv_strides[i],1],padding='SAME'),self.weights['VICI_decoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([1,conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                            conv_dropout = tf.layers.dropout(conv_batchNorm,rate=self.drate)\n",
        "                        else:\n",
        "                            conv_dropout = tf.layers.dropout(conv_post,rate=self.drate)\n",
        "                        conv_pool = tf.nn.max_pool(conv_dropout,ksize=[1, 1, self.maxpool[i], 1],strides=[1, 1, self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([z,tf.reshape(conv_pool, [-1, int(conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)            \n",
        "                if self.by_channel == False:\n",
        "                    conv_pool = tf.reshape(y, shape=[-1, y.shape[1], y.shape[2], 1])\n",
        "                    for i in range(self.n_conv):\n",
        "                        weight_name = 'w_conv_' + str(i)\n",
        "                        bias_name = 'b_conv_' + str(i)\n",
        "                        conv_pre = tf.add(tf.nn.conv2d(conv_pool, self.weights['VICI_decoder'][weight_name],strides=[1,self.conv_strides[i],self.conv_strides[i],1],padding='SAME'),self.weights['VICI_decoder'][bias_name])\n",
        "                        conv_post = self.nonlinearity(conv_pre)\n",
        "                        if self.batch_norm == True:\n",
        "                            conv_batchNorm = tf.nn.batch_normalization(conv_post,tf.Variable(tf.zeros([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),tf.Variable(tf.ones([conv_post.shape[1],conv_post.shape[2],conv_post.shape[3]], dtype=tf.float32)),None,None,0.000001)\n",
        "                        conv_pool = tf.nn.max_pool(conv_batchNorm,ksize=[1, self.maxpool[i], self.maxpool[i], 1],strides=[1, self.pool_strides[i], self.pool_strides[i], 1],padding='SAME')\n",
        "\n",
        "                    fc = tf.concat([z,tf.reshape(conv_pool, [-1, int(conv_pool.shape[1]*conv_pool.shape[2]*conv_pool.shape[3])])],axis=1)\n",
        "            else:\n",
        "                fc = tf.concat([z,y],axis=1)\n",
        "\n",
        "            hidden_dropout = fc\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                hidden_pre = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_decoder'][weight_name]), self.weights['VICI_decoder'][bias_name])\n",
        "                hidden_post = self.nonlinearity(hidden_pre)\n",
        "                if self.batch_norm == True:\n",
        "                    hidden_batchNorm = tf.nn.batch_normalization(hidden_post,tf.Variable(tf.zeros([hidden_post.shape[1]], dtype=tf.float32)),tf.Variable(tf.ones([hidden_post.shape[1]], dtype=tf.float32)),None,None,0.000001)\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_batchNorm,rate=self.drate)\n",
        "                else:\n",
        "                    hidden_dropout = tf.layers.dropout(hidden_post,rate=self.drate)\n",
        "            loc_all = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_decoder']['w_loc']), self.weights['VICI_decoder']['b_loc'])\n",
        "            scale_all = tf.add(tf.matmul(hidden_dropout, self.weights['VICI_decoder']['w_scale']), self.weights['VICI_decoder']['b_scale'])\n",
        "\n",
        "            # split up the output into non-wrapped and wrapped params and apply appropriate activation\n",
        "            loc_nowrap = self.nonlinear_loc_nowrap(tf.boolean_mask(loc_all,self.nowrap_mask,axis=1))\n",
        "            scale_nowrap = self.nonlinear_scale_nowrap(tf.boolean_mask(scale_all,self.nowrap_mask,axis=1))\n",
        "            if np.sum(self.wrap_mask)>0:\n",
        "                loc_wrap = self.nonlinear_loc_wrap(tf.boolean_mask(loc_all,self.wrap_mask,axis=1))\n",
        "                scale_wrap = -1.0*self.nonlinear_scale_wrap(tf.boolean_mask(scale_all,self.wrap_mask,axis=1))\n",
        "                return loc_nowrap, scale_nowrap, loc_wrap, scale_wrap\n",
        "            else:\n",
        "                return loc_nowrap, scale_nowrap\n",
        "\n",
        "    def _create_weights(self):\n",
        "        all_weights = collections.OrderedDict()\n",
        "\n",
        "        # Decoder\n",
        "        with tf.variable_scope(\"VICI_DEC\"):\n",
        "            all_weights['VICI_decoder'] = collections.OrderedDict()\n",
        "            \n",
        "            if self.n_conv is not None:\n",
        "                dummy = 1\n",
        "                for i in range(self.n_conv):\n",
        "                    weight_name = 'w_conv_' + str(i)\n",
        "                    bias_name = 'b_conv_' + str(i)\n",
        "                    # orthogonal init\n",
        "                    if self.weight_init == 'Orthogonal':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.Orthogonal()\n",
        "                        all_weights['VICI_decoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # Variance scaling\n",
        "                    if self.weight_init == 'VarianceScaling':\n",
        "                        shape_init = (self.filter_size[i],dummy*self.n_filters[i])\n",
        "                        initializer = tf.keras.initializers.VarianceScaling()\n",
        "                        all_weights['VICI_decoder'][weight_name] = tf.Variable(tf.reshape(initializer(shape=shape_init),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    # xavier initilization\n",
        "                    if self.weight_init == 'xavier':\n",
        "                        all_weights['VICI_decoder'][weight_name] = tf.Variable(tf.reshape(vae_utils.xavier_init(self.filter_size[i], dummy*self.n_filters[i]),[self.filter_size[i], 1, dummy, self.n_filters[i]]), dtype=tf.float32)\n",
        "                    all_weights['VICI_decoder'][bias_name] = tf.Variable(tf.zeros([self.n_filters[i]], dtype=tf.float32))\n",
        "                    tf.summary.histogram(weight_name, all_weights['VICI_decoder'][weight_name])\n",
        "                    tf.summary.histogram(bias_name, all_weights['VICI_decoder'][bias_name])\n",
        "                    dummy = self.n_filters[i]\n",
        "\n",
        "                total_pool_stride_sum = 0\n",
        "                for j in range(len(self.maxpool)):\n",
        "                    if self.maxpool[j] != 1 and self.pool_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                    else:\n",
        "                        if self.maxpool[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                        if self.pool_strides[j] != 1:\n",
        "                            total_pool_stride_sum += 1\n",
        "                    if self.conv_strides[j] != 1:\n",
        "                        total_pool_stride_sum += 1\n",
        "                if self.by_channel == True:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum))\n",
        "                else:\n",
        "                    fc_input_size = self.n_input1 + int(self.n_input2*self.n_filters[i]/(2**total_pool_stride_sum)*2)\n",
        "            else:\n",
        "                fc_input_size = self.n_input1 + self.n_input2\n",
        "\n",
        "            for i in range(self.n_hlayers):\n",
        "                weight_name = 'w_hidden_' + str(i)\n",
        "                bias_name = 'b_hidden' + str(i)\n",
        "                all_weights['VICI_decoder'][weight_name] = tf.Variable(vae_utils.xavier_init(fc_input_size, self.n_weights[i]), dtype=tf.float32)\n",
        "                all_weights['VICI_decoder'][bias_name] = tf.Variable(tf.zeros([self.n_weights[i]], dtype=tf.float32))\n",
        "                tf.summary.histogram(weight_name, all_weights['VICI_decoder'][weight_name])\n",
        "                tf.summary.histogram(bias_name, all_weights['VICI_decoder'][bias_name])\n",
        "                fc_input_size = self.n_weights[i]\n",
        "            all_weights['VICI_decoder']['w_loc'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_decoder']['b_loc'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_loc', all_weights['VICI_decoder']['w_loc'])\n",
        "            tf.summary.histogram('b_loc', all_weights['VICI_decoder']['b_loc'])\n",
        "            all_weights['VICI_decoder']['w_scale'] = tf.Variable(vae_utils.xavier_init(self.n_weights[-1], self.n_output),dtype=tf.float32)\n",
        "            all_weights['VICI_decoder']['b_scale'] = tf.Variable(tf.zeros([self.n_output], dtype=tf.float32), dtype=tf.float32)\n",
        "            tf.summary.histogram('w_scale', all_weights['VICI_decoder']['w_scale'])\n",
        "            tf.summary.histogram('b_scale', all_weights['VICI_decoder']['b_scale'])\n",
        "            \n",
        "            all_weights['prior_param'] = collections.OrderedDict()\n",
        "        \n",
        "        return all_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtFV8KXBpnI1",
        "colab_type": "text"
      },
      "source": [
        "# Running VItamin on your own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXMU1DTWoep",
        "colab_type": "text"
      },
      "source": [
        "## Generate Bilby Posterior Test Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DHQHxuacdFi",
        "colab_type": "text"
      },
      "source": [
        "### Define bounds and default fixed values for source parameters and search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnTS76ZkjxM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source parameter values to use if chosen to be fixed\n",
        "fixed_vals = {'mass_1':50.0,\n",
        "        'mass_2':50.0,\n",
        "        'mc':None,\n",
        "        'geocent_time':0.0,\n",
        "        'phase':0.0,\n",
        "        'ra':1.375,\n",
        "        'dec':-1.2108,\n",
        "        'psi':0.0,\n",
        "        'theta_jn':0.0,\n",
        "        'luminosity_distance':2000.0,\n",
        "        'a_1':0.0,\n",
        "        'a_2':0.0,\n",
        "\t'tilt_1':0.0,\n",
        "\t'tilt_2':0.0,\n",
        "        'phi_12':0.0,\n",
        "        'phi_jl':0.0,\n",
        "        'det':['H1','L1','V1']}                                                 # feel free to edit this if more or less detectors wanted\n",
        "\n",
        "\n",
        "# Prior bounds on source parameters\n",
        "bounds = {'mass_1_min':35.0, 'mass_1_max':80.0,\n",
        "        'mass_2_min':35.0, 'mass_2_max':80.0,\n",
        "        'M_min':70.0, 'M_max':160.0,\n",
        "        'geocent_time_min':0.15,'geocent_time_max':0.35,\n",
        "        'phase_min':0.0, 'phase_max':2.0*np.pi,\n",
        "        'ra_min':0.0, 'ra_max':2.0*np.pi,\n",
        "        'dec_min':-0.5*np.pi, 'dec_max':0.5*np.pi,\n",
        "        'psi_min':0.0, 'psi_max':2.0*np.pi,\n",
        "        'theta_jn_min':0.0, 'theta_jn_max':np.pi,\n",
        "        'a_1_min':0.0, 'a_1_max':0.0,\n",
        "        'a_2_min':0.0, 'a_2_max':0.0,\n",
        "        'tilt_1_min':0.0, 'tilt_1_max':0.0,\n",
        "        'tilt_2_min':0.0, 'tilt_2_max':0.0,\n",
        "        'phi_12_min':0.0, 'phi_12_max':0.0,\n",
        "        'phi_jl_min':0.0, 'phi_jl_max':0.0,\n",
        "        'luminosity_distance_min':1000.0, 'luminosity_distance_max':3000.0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3BnAAf8jyHw",
        "colab_type": "text"
      },
      "source": [
        "### Define other run parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NaP1BzdwW5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################\n",
        "# Main tunable variables\n",
        "##########################\n",
        "ndata = 256                                                                     # sampling frequency\n",
        "rand_pars = ['mass_1','mass_2','luminosity_distance','geocent_time','phase',\n",
        "                 'theta_jn','psi','ra','dec']                                   # parameters to randomize (those not listed here are fixed otherwise)\n",
        "inf_pars=['luminosity_distance','geocent_time','ra','dec'],                     # parameters to infer\n",
        "batch_size = 64                                                                 # Number training samples shown to neural network per iteration\n",
        "weight_init = 'xavier',                                                         #[xavier,VarianceScaling,Orthogonal] # Network model weight initialization    \n",
        "n_modes=7,                                                                      # number of modes in Gaussian mixture model (ideal 7, but may go higher/lower)\n",
        "initial_training_rate=1e-4,                                                     # initial training rate for ADAM optimiser inference model (inverse reconstruction)\n",
        "batch_norm=True,                                                                # if true, do batch normalization in all layers of neural network\n",
        "\n",
        "# FYI, each item in lists below correspond to each layer in networks (i.e. first item first layer)\n",
        "# pool size and pool stride should be same number in each layer\n",
        "n_filters_r1 = [33, 33, 33],                                                    # number of convolutional filters to use in r1 network (must be divisible by 3)\n",
        "n_filters_r2 = [33, 33, 33],                                                    # number of convolutional filters to use in r2 network (must be divisible by 3)\n",
        "n_filters_q = [33, 33, 33],                                                     # number of convolutional filters to use in q network  (must be divisible by 3)\n",
        "filter_size_r1 = [7,7,7],                                                       # size of convolutional fitlers in r1 network\n",
        "filter_size_r2 = [7,7,7],                                                       # size of convolutional filters in r2 network\n",
        "filter_size_q = [7,7,7],                                                        # size of convolutional filters in q network\n",
        "drate = 0.5,                                                                    # dropout rate to use in fully-connected layers\n",
        "maxpool_r1 = [1,2,1],                                                           # size of maxpooling to use in r1 network\n",
        "conv_strides_r1 = [1,1,1],                                                      # size of convolutional stride to use in r1 network\n",
        "pool_strides_r1 = [1,2,1],                                                      # size of max pool stride to use in r1 network\n",
        "maxpool_r2 = [1,2,1],                                                           # size of max pooling to use in r2 network\n",
        "conv_strides_r2 = [1,1,1],                                                      # size of convolutional stride in r2 network\n",
        "pool_strides_r2 = [1,2,1],                                                      # size of max pool stride in r2 network\n",
        "maxpool_q = [1,2,1],                                                            # size of max pooling to use in q network\n",
        "conv_strides_q = [1,1,1],                                                       # size of convolutional stride to use in q network\n",
        "pool_strides_q = [1,2,1],                                                       # size of max pool stride to use in q network\n",
        "n_fc = 512                                                                      # Number of neurons in fully-connected layers\n",
        "z_dimension=8,                                                                  # number of latent space dimensions of model \n",
        "n_weights_r1 = [n_fc,n_fc],                                                     # number fully-connected layers of encoders and decoders in the r1 model (inverse reconstruction)\n",
        "n_weights_r2 = [n_fc,n_fc],                                                     # number fully-connected layers of encoders and decoders in the r2 model (inverse reconstruction)\n",
        "n_weights_q = [n_fc,n_fc],                                                      # number fully-connected layers of encoders and decoders q model\n",
        "##########################\n",
        "# Main tunable variables\n",
        "##########################\n",
        "\n",
        "#############################\n",
        "# optional tunable variables\n",
        "#############################\n",
        "run_label = 'ozgrav-demo_%ddet_%dpar_%dHz_run1' % (len(fixed_vals['det']),len(rand_pars),ndata) # label of run\n",
        "bilby_results_label = 'ozgrav-demo'                                             # label given to bilby results directory\n",
        "r = 2                                                                           # number (to the power of 2) of test samples to use for testing. r = 2 means you want to use 2^2 (i.e 4) test samples\n",
        "pe_test_num = 256                                                               # total number of test samples available to use in directory\n",
        "tot_dataset_size = int(1e4)                                                     # total number of training samples available to use\n",
        "tset_split = int(1e3)                                                           # number of training samples in each training data file\n",
        "save_interval = int(5e4)                                                        # number of iterations to save model and plot validation results corner plots\n",
        "ref_geocent_time=1126259642.5                                                   # reference gps time (not advised to change this)\n",
        "load_chunk_size = 1e4                                                           # Number of training samples to load in at a time.\n",
        "samplers=['vitamin','dynesty'],                                                 # Bayesian samplers to use when comparing ML results (vitamin is ML approach) dynesty,ptemcee,cpnest,emcee\n",
        "\n",
        "# Directory variables\n",
        "plot_dir=\"results/%s\" % run_label,  # output directory to save results plots\n",
        "train_set_dir='training_sets_%ddet_%dpar_%dHz/tset_tot-%d_split-%d' % (len(fixed_vals['det']),len(rand_pars),ndata,tot_dataset_size,tset_split), # location of training set\n",
        "test_set_dir='test_sets/%s/four_parameter_case/test_waveforms' % bilby_results_label,                                                            # location of test set directory waveforms\n",
        "pe_dir='test_sets/%s/four_parameter_case/test' % bilby_results_label,                                                                            # location of test set directory Bayesian PE samples\n",
        "#############################\n",
        "# optional tunable variables\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG3Z6hk2dYA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function for getting list of parameters that need to be fed into the models\n",
        "def get_params():\n",
        "\n",
        "    # Define dictionary to store values used in rest of code \n",
        "    params = dict(\n",
        "        make_corner_plots = True,                                               # if True, make corner plots\n",
        "        make_kl_plot = True,                                                    # If True, go through kl plotting function\n",
        "        make_pp_plot = True,                                                    # If True, go through pp plotting function\n",
        "        make_loss_plot = False,                                                 # If True, generate loss plot from previous plot data\n",
        "        Make_sky_plot=False,                                                    # If True, generate sky plots on corner plots\n",
        "        hyperparam_optim = False,                                               # optimize hyperparameters for model during training using gaussian process minimization\n",
        "        resume_training=False,                                                  # if True, resume training of a model from saved checkpoint\n",
        "        load_by_chunks = True,                                                  # if True, load training samples by a predefined chunk size rather than all at once\n",
        "        ramp = True,                                                            # if true, apply linear ramp to KL loss\n",
        "        print_values=True,                                                      # optionally print loss values every report interval\n",
        "        by_channel = True,                                                      # if True, do convolutions as seperate 1-D channels, if False, stack training samples as 2-D images (n_detectors,(duration*sampling_frequency))\n",
        "        load_plot_data=False,                                                   # Plotting data which has already been generated\n",
        "        doPE = True,                                                            # if True then do bilby PE when generating new testing samples (not advised to change this)\n",
        "        gpu_num=0,                                                              # gpu number run is running on\n",
        "        ndata = ndata,                                                          \n",
        "        run_label=run_label,                                                    \n",
        "        bilby_results_label=bilby_results_label,                                \n",
        "        tot_dataset_size = tot_dataset_size,                                    \n",
        "        tset_split = tset_split,                                                \n",
        "        plot_dir=plot_dir,\n",
        "\n",
        "        # Gaussian Process automated hyperparameter tunning variables\n",
        "        hyperparam_optim_stop = int(1.5e6),                                     # stopping iteration of hyperparameter optimizer per call (ideally 1.5 million) \n",
        "        hyperparam_n_call = 30,                                                 # number of hyperparameter optimization calls (ideally 30)\n",
        "        load_chunk_size = load_chunk_size,                                      \n",
        "        load_iteration = int((load_chunk_size * 25)/batch_size),                # How often to load another chunk of training samples\n",
        "        weight_init = weight_init[0],                                           \n",
        "        n_samples = 1000,                                                       # number of posterior samples to save per reconstruction upon inference (default 3000) \n",
        "        num_iterations=int(1e7)+1,                                              # total number of iterations before ending training of model\n",
        "        initial_training_rate=initial_training_rate[0],                         \n",
        "        batch_size=batch_size,                                                  \n",
        "        batch_norm=batch_norm,                                                  \n",
        "        report_interval=500,                                                    # interval at which to save objective function values and optionally print info during training\n",
        "        n_modes=n_modes[0],                                                     \n",
        "\n",
        "        # FYI, each item in lists below correspond to each layer in networks (i.e. first item first layer)\n",
        "        # pool size and pool stride should be same number in each layer\n",
        "        n_filters_r1 = n_filters_r1[0],                                         \n",
        "        n_filters_r2 = n_filters_r2[0],                                         \n",
        "        n_filters_q = n_filters_q[0],                                           \n",
        "        filter_size_r1 = filter_size_r1[0],                                     \n",
        "        filter_size_r2 = filter_size_r2[0],                                     \n",
        "        filter_size_q = filter_size_q[0],                                       \n",
        "        drate = drate[0],                                                       \n",
        "        maxpool_r1 = maxpool_r1[0],                                             \n",
        "        conv_strides_r1 = conv_strides_r1[0],                                   \n",
        "        pool_strides_r1 = pool_strides_r1[0],                                   \n",
        "        maxpool_r2 = maxpool_r2[0],                                             \n",
        "        conv_strides_r2 = conv_strides_r2[0],                                   \n",
        "        pool_strides_r2 = pool_strides_r2[0],                                   \n",
        "        maxpool_q = maxpool_q[0],                                               \n",
        "        conv_strides_q = conv_strides_q[0],                                     \n",
        "        pool_strides_q = pool_strides_q[0],                                     \n",
        "        ramp_start = 1e4,                                                       # starting iteration of KL divergence ramp (if using)\n",
        "        ramp_end = 1e5,                                                         # ending iteration of KL divergence ramp (if using)\n",
        "        save_interval=save_interval,                                            \n",
        "        plot_interval=save_interval,                                            \n",
        "        z_dimension=z_dimension[0],                                              \n",
        "        n_weights_r1 = n_weights_r1[0],                                         \n",
        "        n_weights_r2 = n_weights_r2[0],                                         \n",
        "        n_weights_q = n_weights_q[0],                                           \n",
        "        duration = 1.0,                                                         # length of training/validation/test sample time series in seconds (haven't tried using at any other value than 1s)\n",
        "        r = r,                                                                  \n",
        "        rand_pars=rand_pars,                                                    \n",
        "        corner_parnames = ['m_{1}\\,(\\mathrm{M}_{\\odot})','m_{2}\\,(\\mathrm{M}_{\\odot})','d_{\\mathrm{L}}\\,(\\mathrm{Mpc})','t_{0}\\,(\\mathrm{seconds})','{\\phi}','\\Theta_{jn}\\,(\\mathrm{rad})','{\\psi}',r'{\\alpha}\\,(\\mathrm{rad})','{\\delta}\\,(\\mathrm{rad})'], # latex source parameter labels for plotting\n",
        "        cornercorner_parnames = ['$m_{1}\\,(\\mathrm{M}_{\\odot})$','$m_{2}\\,(\\mathrm{M}_{\\odot})$','$d_{\\mathrm{L}}\\,(\\mathrm{Mpc})$','$t_{0}\\,(\\mathrm{seconds})$','${\\phi}$','$\\Theta_{jn}\\,(\\mathrm{rad})$','${\\psi}$',r'${\\alpha}\\,(\\mathrm{rad})$','${\\delta}\\,(\\mathrm{rad})$'], # latex source parameter labels for plotting\n",
        "        ref_geocent_time=ref_geocent_time,                                      \n",
        "        training_data_seed=43,                                                  # tensorflow training random seed number\n",
        "        testing_data_seed=44,                                                   # tensorflow testing random seed number\n",
        "        wrap_pars=['phase','psi','ra'],                                         # Parameters to apply Von Mises wrapping on (not advised to change) \n",
        "        inf_pars=inf_pars,                                                      \n",
        "        train_set_dir=train_set_dir,\n",
        "        test_set_dir=test_set_dir,\n",
        "        pe_dir=pe_dir,\n",
        "        KL_cycles = 1,                                                          # number of cycles to repeat for the KL approximation\n",
        "        samplers=samplers,                                                      \n",
        "    )\n",
        "    return params\n",
        "\n",
        "\n",
        "# Save training/test parameters of run\n",
        "params=get_params()\n",
        "params['plot_dir']=params['plot_dir'][0]\n",
        "params['train_set_dir']=params['train_set_dir'][0]\n",
        "params['test_set_dir']=params['test_set_dir'][0]\n",
        "params['pe_dir']=params['pe_dir'][0]\n",
        "params['inf_pars']=params['inf_pars'][0]\n",
        "params['samplers']=params['samplers'][0]\n",
        "f = open(\"params_%s.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(params) )\n",
        "f.close()\n",
        "f = open(\"params_%s_bounds.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(bounds) )\n",
        "f.close()\n",
        "f = open(\"params_%s_fixed_vals.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(fixed_vals) )\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbVDzVuBciLz",
        "colab_type": "text"
      },
      "source": [
        "### Generate Requested number of testing samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1oK3PUnR72q",
        "colab_type": "text"
      },
      "source": [
        "Feel free to use the below command in your own time to generate more testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL7wQwyMM3Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --gen_test True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhhXcM5NW92Q",
        "colab_type": "text"
      },
      "source": [
        "## Generate Traning Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3euU5D6zXCGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --gen_train True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XmOPmHJ3PT1u"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TN5F_GVfPkWQ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgMNjoMhPrHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --train True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeE98hRnXadJ",
        "colab_type": "text"
      },
      "source": [
        "## Compare Bilby vs. VItamin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sGBzCe9AyB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --test True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DsnK_tPXepg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T4N8EJLXgu-",
        "colab_type": "text"
      },
      "source": [
        "# Further Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maLy6ASgXkQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}