{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OzGrav_VItamin_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hagabbar/OzGrav_demo/blob/master/OzGrav_VItamin_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzz4HG9RKqfV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "4e27cc6a-d3fb-4e16-e889-716c7bda868f"
      },
      "source": [
        "!git pull "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)   \rUnpacking objects:  66% (2/3)   \rUnpacking objects: 100% (3/3)   \rUnpacking objects: 100% (3/3), done.\n",
            "From https://github.com/hagabbar/OzGrav_demo\n",
            "   a82ae1b..c323a78  master     -> origin/master\n",
            "Updating a82ae1b..c323a78\n",
            "Fast-forward\n",
            " VICI_code_usage_example.py | 12 \u001b[32m+++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 11 insertions(+), 1 deletion(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdoMTkVPVL6E",
        "colab_type": "text"
      },
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHVnnfXANukP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "ac16c9a6-bd7f-4c26-eb52-c498178834f8"
      },
      "source": [
        "!git clone https://github.com/hagabbar/OzGrav_demo.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OzGrav_demo'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 79 (delta 21), reused 57 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tA7BWiUN1NK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "d6475651-8137-4da9-81ab-764f0f225acf"
      },
      "source": [
        "%cd OzGrav_demo/\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/OzGrav_demo\n",
            "bilby_pe.py         OzGrav_VItamin_demo.ipynb  \u001b[0m\u001b[01;34mtest_sets\u001b[0m/\n",
            "LICENSE             plotsky.py                 \u001b[01;34mtraining_sets_3det_9par_256Hz\u001b[0m/\n",
            "make_test_plots.py  plots.py                   VICI_code_usage_example.py\n",
            "\u001b[01;34mModels\u001b[0m/             README.md\n",
            "\u001b[01;34mNeural_Networks\u001b[0m/    requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI04ILUsRgRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naVHRD3ugIol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZdzHYaSVPvb",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "This tutorial is based on the paper Bayesian Parameter Estimation using Conditional Variational Autoencoders for Gravitational-wave Astronomy (https://arxiv.org/abs/1909.06296). In this demo, we will show how a form of conditional variational autoencoders work and how to reproduce Bayesian posteriors several orders of magnitude faster than exisitng Bayesian techniques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBHd896AVRs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXMU1DTWoep",
        "colab_type": "text"
      },
      "source": [
        "# Generate Bilby Posteriors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DHQHxuacdFi",
        "colab_type": "text"
      },
      "source": [
        "## Define bounds and default fixed values for source parameters and search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnTS76ZkjxM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source parameter values to use if chosen to be fixed\n",
        "fixed_vals = {'mass_1':50.0,\n",
        "        'mass_2':50.0,\n",
        "        'mc':None,\n",
        "        'geocent_time':0.0,\n",
        "        'phase':0.0,\n",
        "        'ra':1.375,\n",
        "        'dec':-1.2108,\n",
        "        'psi':0.0,\n",
        "        'theta_jn':0.0,\n",
        "        'luminosity_distance':2000.0,\n",
        "        'a_1':0.0,\n",
        "        'a_2':0.0,\n",
        "\t'tilt_1':0.0,\n",
        "\t'tilt_2':0.0,\n",
        "        'phi_12':0.0,\n",
        "        'phi_jl':0.0,\n",
        "        'det':['H1','L1','V1']}                                                 # feel free to edit this if more or less detectors wanted\n",
        "\n",
        "\n",
        "# Prior bounds on source parameters\n",
        "bounds = {'mass_1_min':35.0, 'mass_1_max':80.0,\n",
        "        'mass_2_min':35.0, 'mass_2_max':80.0,\n",
        "        'M_min':70.0, 'M_max':160.0,\n",
        "        'geocent_time_min':0.15,'geocent_time_max':0.35,\n",
        "        'phase_min':0.0, 'phase_max':2.0*np.pi,\n",
        "        'ra_min':0.0, 'ra_max':2.0*np.pi,\n",
        "        'dec_min':-0.5*np.pi, 'dec_max':0.5*np.pi,\n",
        "        'psi_min':0.0, 'psi_max':2.0*np.pi,\n",
        "        'theta_jn_min':0.0, 'theta_jn_max':np.pi,\n",
        "        'a_1_min':0.0, 'a_1_max':0.0,\n",
        "        'a_2_min':0.0, 'a_2_max':0.0,\n",
        "        'tilt_1_min':0.0, 'tilt_1_max':0.0,\n",
        "        'tilt_2_min':0.0, 'tilt_2_max':0.0,\n",
        "        'phi_12_min':0.0, 'phi_12_max':0.0,\n",
        "        'phi_jl_min':0.0, 'phi_jl_max':0.0,\n",
        "        'luminosity_distance_min':1000.0, 'luminosity_distance_max':3000.0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3BnAAf8jyHw",
        "colab_type": "text"
      },
      "source": [
        "## Define other run parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG3Z6hk2dYA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function for getting list of parameters that need to be fed into the models\n",
        "def get_params():\n",
        "\n",
        "    ##########################\n",
        "    # Main tunable variables\n",
        "    ##########################\n",
        "    ndata = 256                                                                 # sampling frequency\n",
        "    rand_pars = ['mass_1','mass_2','luminosity_distance','geocent_time','phase',\n",
        "                 'theta_jn','psi','ra','dec']                                   # parameters to randomize\n",
        "    inf_pars=['luminosity_distance','geocent_time','ra','dec'],                 # parameters to infer\n",
        "    batch_size = 64                                                             # Number training samples shown to neural network per iteration\n",
        "    weight_init = 'xavier',                                                     #[xavier,VarianceScaling,Orthogonal] # Network model weight initialization    \n",
        "    n_modes=7,                                                                  # number of modes in Gaussian mixture model (ideal 7, but may go higher/lower)\n",
        "    initial_training_rate=1e-4,                                                 # initial training rate for ADAM optimiser inference model (inverse reconstruction)\n",
        "    batch_norm=True,                                                            # if true, do batch normalization in all layers of neural network\n",
        "\n",
        "    # FYI, each item in lists below correspond to each layer in networks (i.e. first item first layer)\n",
        "    # pool size and pool stride should be same number in each layer\n",
        "    n_filters_r1 = [33, 33, 33],                                                # number of convolutional filters to use in r1 network\n",
        "    n_filters_r2 = [33, 33, 33],                                                # number of convolutional filters to use in r2 network\n",
        "    n_filters_q = [33, 33, 33],                                                 # number of convolutional filters to use in q network\n",
        "    filter_size_r1 = [7,7,7],                                                   # size of convolutional fitlers in r1 network\n",
        "    filter_size_r2 = [7,7,7],                                                   # size of convolutional filters in r2 network\n",
        "    filter_size_q = [7,7,7],                                                    # size of convolutional filters in q network\n",
        "    drate = 0.5,                                                                # dropout rate to use in fully-connected layers\n",
        "    maxpool_r1 = [1,2,1],                                                       # size of maxpooling to use in r1 network\n",
        "    conv_strides_r1 = [1,1,1],                                                  # size of convolutional stride to use in r1 network\n",
        "    pool_strides_r1 = [1,2,1],                                                  # size of max pool stride to use in r1 network\n",
        "    maxpool_r2 = [1,2,1],                                                       # size of max pooling to use in r2 network\n",
        "    conv_strides_r2 = [1,1,1],                                                  # size of convolutional stride in r2 network\n",
        "    pool_strides_r2 = [1,2,1],                                                  # size of max pool stride in r2 network\n",
        "    maxpool_q = [1,2,1],                                                        # size of max pooling to use in q network\n",
        "    conv_strides_q = [1,1,1],                                                   # size of convolutional stride to use in q network\n",
        "    pool_strides_q = [1,2,1],                                                   # size of max pool stride to use in q network\n",
        "    n_fc = 512                                                                  # Number of neurons in fully-connected layers\n",
        "    z_dimension=8,                                                              # number of latent space dimensions of model \n",
        "    n_weights_r1 = [n_fc,n_fc],                                                 # number fully-connected layers of encoders and decoders in the r1 model (inverse reconstruction)\n",
        "    n_weights_r2 = [n_fc,n_fc],                                                 # number fully-connected layers of encoders and decoders in the r2 model (inverse reconstruction)\n",
        "    n_weights_q = [n_fc,n_fc],                                                  # number fully-connected layers of encoders and decoders q model\n",
        "    ##########################\n",
        "    # Main tunable variables\n",
        "    ##########################\n",
        "\n",
        "    #############################\n",
        "    # optional tunable variables\n",
        "    #############################\n",
        "    run_label = 'ozgrav-demo_%ddet_%dpar_%dHz_run1' % (len(fixed_vals['det']),len(rand_pars),ndata) # label of run\n",
        "    bilby_results_label = 'ozgrav-demo'                                         # label given to bilby results directory\n",
        "    r = 2                                                                       # number (to the power of 2) of test samples to use for testing\n",
        "    pe_test_num = 256                                                           # total number of test samples available to use in directory\n",
        "    tot_dataset_size = int(1e4)                                                 # total number of training samples available to use\n",
        "    tset_split = int(1e3)                                                       # number of training samples in each training data file\n",
        "    save_interval = int(5e4)                                                    # number of iterations to save model and plot validation results corner plots\n",
        "    ref_geocent_time=1126259642.5                                               # reference gps time (not advised to change this)\n",
        "    load_chunk_size = 1e4                                                       # Number of training samples to load in at a time.\n",
        "    samplers=['vitamin','dynesty'],                                             # Bayesian samplers to use when comparing ML results (vitamin is ML approach) dynesty,ptemcee,cpnest,emcee\n",
        "\n",
        "    # Directory variables\n",
        "    plot_dir=\"results/%s\" % run_label,  # output directory to save results plots\n",
        "    train_set_dir='training_sets_%ddet_%dpar_%dHz/tset_tot-%d_split-%d' % (len(fixed_vals['det']),len(rand_pars),ndata,tot_dataset_size,tset_split), # location of training set\n",
        "    test_set_dir='test_sets/%s/four_parameter_case/test_waveforms' % bilby_results_label,                                                            # location of test set directory waveforms\n",
        "    pe_dir='test_sets/%s/four_parameter_case/test' % bilby_results_label,                                                                            # location of test set directory Bayesian PE samples\n",
        "\n",
        "    # Define dictionary to store values used in rest of code \n",
        "    params = dict(\n",
        "        make_corner_plots = True,                                               # if True, make corner plots\n",
        "        make_kl_plot = True,                                                    # If True, go through kl plotting function\n",
        "        make_pp_plot = True,                                                    # If True, go through pp plotting function\n",
        "        make_loss_plot = False,                                                 # If True, generate loss plot from previous plot data\n",
        "        Make_sky_plot=False,                                                    # If True, generate sky plots on corner plots\n",
        "        hyperparam_optim = False,                                               # optimize hyperparameters for model during training using gaussian process minimization\n",
        "        resume_training=False,                                                  # if True, resume training of a model from saved checkpoint\n",
        "        load_by_chunks = True,                                                  # if True, load training samples by a predefined chunk size rather than all at once\n",
        "        ramp = True,                                                            # if true, apply linear ramp to KL loss\n",
        "        print_values=True,                                                      # optionally print loss values every report interval\n",
        "        by_channel = True,                                                      # if True, do convolutions as seperate 1-D channels, if False, stack training samples as 2-D images (n_detectors,(duration*sampling_frequency))\n",
        "        load_plot_data=False,                                                   # Plotting data which has already been generated\n",
        "        doPE = True,                                                            # if True then do bilby PE when generating new testing samples (not advised to change this)\n",
        "        gpu_num=0,                                                              # gpu number run is running on\n",
        "        ndata = ndata,                                                          \n",
        "        run_label=run_label,                                                    \n",
        "        bilby_results_label=bilby_results_label,                                \n",
        "        tot_dataset_size = tot_dataset_size,                                    \n",
        "        tset_split = tset_split,                                                \n",
        "        plot_dir=plot_dir,\n",
        "        # Gaussian Process automated hyperparameter tunning variables\n",
        "        hyperparam_optim_stop = int(1.5e6),                                     # stopping iteration of hyperparameter optimizer per call (ideally 1.5 million) \n",
        "        hyperparam_n_call = 30,                                                 # number of hyperparameter optimization calls (ideally 30)\n",
        "        load_chunk_size = load_chunk_size,                                      \n",
        "        load_iteration = int((load_chunk_size * 25)/batch_size),                # How often to load another chunk of training samples\n",
        "        weight_init = weight_init[0],                                           \n",
        "        n_samples = 1000,                                                       # number of posterior samples to save per reconstruction upon inference (default 3000) \n",
        "        num_iterations=int(1e7)+1,                                              # total number of iterations before ending training of model\n",
        "        initial_training_rate=initial_training_rate[0],                         \n",
        "        batch_size=batch_size,                                                  \n",
        "        batch_norm=batch_norm,                                                  \n",
        "        report_interval=500,                                                    # interval at which to save objective function values and optionally print info during training\n",
        "        n_modes=n_modes[0],                                                     \n",
        "        # FYI, each item in lists below correspond to each layer in networks (i.e. first item first layer)\n",
        "        # pool size and pool stride should be same number in each layer\n",
        "        n_filters_r1 = n_filters_r1[0],                                         \n",
        "        n_filters_r2 = n_filters_r2[0],                                         \n",
        "        n_filters_q = n_filters_q[0],                                           \n",
        "        filter_size_r1 = filter_size_r1[0],                                     \n",
        "        filter_size_r2 = filter_size_r2[0],                                     \n",
        "        filter_size_q = filter_size_q[0],                                       \n",
        "        drate = drate[0],                                                       \n",
        "        maxpool_r1 = maxpool_r1[0],                                             \n",
        "        conv_strides_r1 = conv_strides_r1[0],                                   \n",
        "        pool_strides_r1 = pool_strides_r1[0],                                   \n",
        "        maxpool_r2 = maxpool_r2[0],                                             \n",
        "        conv_strides_r2 = conv_strides_r2[0],                                   \n",
        "        pool_strides_r2 = pool_strides_r2[0],                                   \n",
        "        maxpool_q = maxpool_q[0],                                               \n",
        "        conv_strides_q = conv_strides_q[0],                                     \n",
        "        pool_strides_q = pool_strides_q[0],                                     \n",
        "        ramp_start = 1e4,                                                       # starting iteration of KL divergence ramp (if using)\n",
        "        ramp_end = 1e5,                                                         # ending iteration of KL divergence ramp (if using)\n",
        "        save_interval=save_interval,                                            \n",
        "        plot_interval=save_interval,                                            \n",
        "        z_dimension=z_dimension[0],                                              \n",
        "        n_weights_r1 = n_weights_r1[0],                                         \n",
        "        n_weights_r2 = n_weights_r2[0],                                         \n",
        "        n_weights_q = n_weights_q[0],                                           \n",
        "        duration = 1.0,                                                         # length of training/validation/test sample time series in seconds (haven't tried using at any other value than 1s)\n",
        "        r = r,                                                                  \n",
        "        rand_pars=rand_pars,                                                    \n",
        "        corner_parnames = ['m_{1}\\,(\\mathrm{M}_{\\odot})','m_{2}\\,(\\mathrm{M}_{\\odot})','d_{\\mathrm{L}}\\,(\\mathrm{Mpc})','t_{0}\\,(\\mathrm{seconds})','{\\phi}','\\Theta_{jn}\\,(\\mathrm{rad})','{\\psi}',r'{\\alpha}\\,(\\mathrm{rad})','{\\delta}\\,(\\mathrm{rad})'], # latex source parameter labels for plotting\n",
        "        cornercorner_parnames = ['$m_{1}\\,(\\mathrm{M}_{\\odot})$','$m_{2}\\,(\\mathrm{M}_{\\odot})$','$d_{\\mathrm{L}}\\,(\\mathrm{Mpc})$','$t_{0}\\,(\\mathrm{seconds})$','${\\phi}$','$\\Theta_{jn}\\,(\\mathrm{rad})$','${\\psi}$',r'${\\alpha}\\,(\\mathrm{rad})$','${\\delta}\\,(\\mathrm{rad})$'], # latex source parameter labels for plotting\n",
        "        ref_geocent_time=ref_geocent_time,                                      \n",
        "        training_data_seed=43,                                                  # tensorflow training random seed number\n",
        "        testing_data_seed=44,                                                   # tensorflow testing random seed number\n",
        "        wrap_pars=['phase','psi','ra'],                                         # Parameters to apply Von Mises wrapping on (not advised to change) \n",
        "        inf_pars=inf_pars,                                                      \n",
        "        train_set_dir=train_set_dir,\n",
        "        test_set_dir=test_set_dir,\n",
        "        pe_dir=pe_dir,\n",
        "        # attempt_to_fix_astropy_bug is default directory\n",
        "        KL_cycles = 1,                                                          # number of cycles to repeat for the KL approximation\n",
        "        samplers=samplers,                                                      \n",
        "        #############################\n",
        "        # optional tunable variables\n",
        "        #############################\n",
        "    )\n",
        "    return params\n",
        "\n",
        "\n",
        "# Save training/test parameters of run\n",
        "params=get_params()\n",
        "params['plot_dir']=params['plot_dir'][0]\n",
        "params['train_set_dir']=params['train_set_dir'][0]\n",
        "params['test_set_dir']=params['test_set_dir'][0]\n",
        "params['pe_dir']=params['pe_dir'][0]\n",
        "params['inf_pars']=params['inf_pars'][0]\n",
        "params['samplers']=params['samplers'][0]\n",
        "f = open(\"params_%s.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(params) )\n",
        "f.close()\n",
        "f = open(\"params_%s_bounds.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(bounds) )\n",
        "f.close()\n",
        "f = open(\"params_%s_fixed_vals.txt\" % params['run_label'],\"w\")\n",
        "f.write( str(fixed_vals) )\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbVDzVuBciLz",
        "colab_type": "text"
      },
      "source": [
        "## Generate Requested number of testing samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1oK3PUnR72q",
        "colab_type": "text"
      },
      "source": [
        "Feel free to use the below command in your own time to generate more testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL7wQwyMM3Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --gen_test True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCo6hAuTWvQf",
        "colab_type": "text"
      },
      "source": [
        "# Intro to VItamin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAYQo8-oW9LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhhXcM5NW92Q",
        "colab_type": "text"
      },
      "source": [
        "# Generate Traning Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3euU5D6zXCGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --gen_train True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XmOPmHJ3PT1u"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TN5F_GVfPkWQ"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgMNjoMhPrHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --train True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeE98hRnXadJ",
        "colab_type": "text"
      },
      "source": [
        "# Compare Bilby vs. VItamin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sGBzCe9AyB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python VICI_code_usage_example.py --test True --params_file params_ozgrav-demo_3det_9par_256Hz_run1.txt --params_file_bounds params_ozgrav-demo_3det_9par_256Hz_run1_bounds.txt --params_file_fixed_vals params_ozgrav-demo_3det_9par_256Hz_run1_fixed_vals.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DsnK_tPXepg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T4N8EJLXgu-",
        "colab_type": "text"
      },
      "source": [
        "# Further Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maLy6ASgXkQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}